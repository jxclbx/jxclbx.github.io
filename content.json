{"meta":{"title":"jxclbx的小站","subtitle":"","description":"诶嘿","author":"Jxclbx","url":"http://jxclbx.github.io","root":"/"},"pages":[{"title":"","date":"2023-05-08T05:44:55.926Z","updated":"2023-04-28T05:47:48.000Z","comments":true,"path":"404.html","permalink":"http://jxclbx.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2023-05-08T05:44:55.943Z","updated":"2023-04-28T05:53:46.000Z","comments":true,"path":"categories/index.html","permalink":"http://jxclbx.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-05-08T05:44:55.935Z","updated":"2023-04-28T05:53:42.000Z","comments":true,"path":"about/index.html","permalink":"http://jxclbx.github.io/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"我的朋友们","date":"2023-05-08T05:44:56.031Z","updated":"2023-04-28T05:53:58.000Z","comments":true,"path":"friends/index.html","permalink":"http://jxclbx.github.io/friends/index.html","excerpt":"友情链接没有友链是万万不行滴！ 虽然博客是我自己写的，但是感谢他们对我一路的资瓷~","text":"友情链接没有友链是万万不行滴！ 虽然博客是我自己写的，但是感谢他们对我一路的资瓷~"}],"posts":[{"title":"测试自动图床","slug":"测试自动图床","date":"2023-08-27T17:40:04.000Z","updated":"2023-08-27T17:49:51.660Z","comments":true,"path":"2023/08/28/测试自动图床/","link":"","permalink":"http://jxclbx.github.io/2023/08/28/%E6%B5%8B%E8%AF%95%E8%87%AA%E5%8A%A8%E5%9B%BE%E5%BA%8A/","excerpt":"","text":"操，怎么就失败了呢，再试试","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"python","slug":"python","permalink":"http://jxclbx.github.io/tags/python/"},{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/tags/HTML/"}]},{"title":"我回来了","slug":"我回来了","date":"2023-08-27T16:22:19.000Z","updated":"2023-08-27T17:44:45.253Z","comments":true,"path":"2023/08/28/我回来了/","link":"","permalink":"http://jxclbx.github.io/2023/08/28/%E6%88%91%E5%9B%9E%E6%9D%A5%E4%BA%86/","excerpt":"","text":"是的没错，很久没有更新之后，是时候来审视这个网站的作用。首先它部署在云端，让我回忆起来往昔更加容易一些……其实也并不容易，没有配图的文字确实是十分干燥，五月的事情没有配图，确实已经忘记的一干二净。其次呢，框架式的布局省去了一些排版上的麻烦，这是废话。为了把这些特性发扬出去，就需要一个提交网站的脚本……实现起来有一些技术难度，但我相信是可以攻克的，这样每时每刻想写的内容就可以及时地被记录下来，上传到图床。 明天开学，下学期能不能做的好一点呢？","categories":[{"name":"浮生","slug":"浮生","permalink":"http://jxclbx.github.io/categories/%E6%B5%AE%E7%94%9F/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://jxclbx.github.io/tags/%E6%97%A5%E8%AE%B0/"}]},{"title":"机器视觉技术期末复习（4）","slug":"机器视觉技术期末复习（4）","date":"2023-06-06T09:15:34.000Z","updated":"2023-06-07T14:54:27.209Z","comments":true,"path":"2023/06/06/机器视觉技术期末复习（4）/","link":"","permalink":"http://jxclbx.github.io/2023/06/06/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%884%EF%BC%89/","excerpt":"","text":"传统特征检测 Moravec 计算图像的梯度 ：首先计算图像在x和y方向上的梯度Ix和Iy。这可以通过卷积操作来完成，常用的梯度计算核包括Sobel核和Prewitt核。 计算Harris矩阵 ：根据梯度Ix和Iy，计算每个像素点的Harris矩阵M： 其中，求和是在一个小窗口内进行的。 计算响应函数 ：对于每个像素点，计算其Harris响应函数R： M=[sum(Ix∗Ix) sum(Ix∗Iy)sum(Iy∗Ix) sum(Iy∗Iy)]M = [sum(Ix * Ix) \\ sum(Ix * Iy)\\\\ sum(Iy * Ix) \\ sum(Iy * Iy)] M=[sum(Ix∗Ix) sum(Ix∗Iy)sum(Iy∗Ix) sum(Iy∗Iy)] 其中，det(M)是矩阵M的行列式，trace(M)是矩阵M的迹（即对角线上元素的和），k是敏感度系数，一般取值为0.04-0.06。 R=det(M)−k∗(trace(M))2R = det(M) - k * (trace(M))^2 R=det(M)−k∗(trace(M))2 设定阈值并筛选角点 ：设定一个阈值，如果某个像素点的R值大于这个阈值，那么就将这个像素点标记为角点。 非极大值抑制 ：为了避免角点过于密集，可以在每个角点周围的一定范围内，只保留R值最大的角点，其他的角点都被抑制。 Harris 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566clear;filename = &#x27;Lena.jpg&#x27;;X = imread(filename); Info = imfinfo(filename); if (Info.BitDepth &gt; 8) f = rgb2gray(X);end%计算图像亮度f(x,y)在点(x,y)处的梯度-----------------------------------------------ori_im = double(f) / 255; fx = [-2 -1 0 1 2]; % x方向梯度算子Ix = filter2(fx, ori_im); fy = [-2; -1; 0; 1; 2]; % y方向梯度算子Iy = filter2(fy, ori_im); % y方向滤波%构造自相关矩阵---------------------------------------------------------------Ix2 = Ix .^ 2;Iy2 = Iy .^ 2;Ixy = Ix .* Iy;clear Ix;clear Iy;h= fspecial(&#x27;gaussian&#x27;, [7 7], 2); % 产生7*7的高斯窗函数，sigma=2Ix2 = filter2(h,Ix2);Iy2 = filter2(h,Iy2);Ixy = filter2(h,Ixy);%提取特征点---------------------------------------------------------------height = size(ori_im, 1);width = size(ori_im, 2);result = zeros(height, width); % 纪录角点位置，角点处值为1R = zeros(height, width);Rmax = 0; % 图像中最大的R值k = 0.06;for i = 1 : height for j = 1 : width M = [Ix2(i, j) Ixy(i, j); Ixy(i, j) Iy2(i, j)]; R(i,j) = det(M) - k * (trace(M)) ^ 2; % 计算R if R(i,j) &gt; Rmax Rmax = R(i, j); end endendT = 0.01 * Rmax;%固定阈值，当R(i, j) &gt; T时，则被判定为候选角点%进行局部非极大值抑制-------------------------------------cnt = 0;for i = 2 : height-1 for j = 2 : width-1 if (R(i, j) &gt; T &amp;&amp; R(i, j) &gt; R(i-1, j-1) &amp;&amp; R(i, j) &gt; R(i-1, j) &amp;&amp; R(i, j) &gt; R(i-1, j+1) &amp;&amp; R(i, j) &gt; R(i, j-1) &amp;&amp; ... R(i, j) &gt; R(i, j+1) &amp;&amp; R(i, j) &gt; R(i+1, j-1) &amp;&amp; R(i, j) &gt; R(i+1, j) &amp;&amp; R(i, j) &gt; R(i+1, j+1)) result(i, j) = 1; cnt = cnt+1; end endendfigure,imshow(ori_im); 优点 计算简单 ：Harris角点检测只涉及到一些基本的图像处理操作，如梯度计算、平滑等，并且响应函数的计算也比较直观，所以计算相对简单。 提取的点特征均匀且合理 ：由于Harris算子基于梯度的变化来寻找角点，所以它可以在整个图像范围内均匀地找到角点。并且它只会找到那些在多个方向上灰度变化都比较大的点，这些点一般都是图像中的显著特征，比如物体的边缘或者角落。 稳定 ：Harris算子对图像的旋转、亮度变化和噪声影响不敏感。这是因为它基于梯度的变化来检测角点，而梯度的变化是相对稳定的，不会因为图像的旋转、亮度变化或噪声的影响而大幅度改变。视点变换对Harris的影响也较小，因为不同视点下，同一物体的角点位置基本不变。 局限性 对尺度很敏感，不具有尺度不变性 ：Harris角点检测依赖于固定尺度的窗口来计算梯度和响应函数。如果图像的尺度变化了（比如，物体离摄像机的距离发生了变化），那么同一物体的角点可能就无法被正确地检测出来。这是因为在不同尺度下，物体的角点可能出现在Harris算子窗口的不同位置。 提取的角点精度是像素级的 ：Harris角点检测基于像素级的梯度计算，所以它提取出来的角点的位置也是像素级的，无法达到亚像素级的精度。如果需要更高的精度，可能需要使用其他方法，比如通过插值来提高角点位置的精度。 需要设计角点匹配算法 ：Harris角点检测只能提取出角点，但是如果要使用这些角点进行图像配准或物体识别等任务，还需要设计角点的描述子和匹配算法。而这些描述子和匹配算法的设计可能会比较复杂，也可能会受到图像的噪声、尺度变化等因素的影响 SIFT 构造金字塔 计算 doubleimagesize参数对noctave的影响 ：SIFT算法在构建尺度空间时，会先对图像进行一次上采样，即将图像大小扩大一倍，然后再逐层下采样。这一步是为了更好地检测尺度空间中的特征点。如果 doubleimagesize参数设置为1，就会进行这一步上采样操作。如果设置为0，则不进行上采样操作，直接从原始图像大小开始进行尺度空间的构建。因此，doubleimagesize参数的设置会影响到第一个octave的图像大小，从而影响到后续所有octave的图像大小。 noctave的firstoctave是什么意思 ：firstoctave是指开始进行尺度空间构建的octave的编号。如果 doubleimagesize参数设置为1，即进行了一次上采样，那么 firstoctave的值就是-1。如果 doubleimagesize参数设置为0，即没有进行上采样，那么 firstoctave的值就是0。 doubleimagesize有什么用 ：doubleimagesize参数的主要作用就是决定是否对图像进行一次上采样操作。进行上采样操作可以使SIFT算法更好地检测到尺度空间中的特征点，尤其是在图像的尺度较小的情况下，可以提高特征点的检测精度。 这个公式计算的是OpenCV的SIFT算法中的octaves（层次）数量，其中 base.cols和 base.rows分别表示图像的列数（宽度）和行数（高度），firstOctave是开始进行尺度空间构建的octave的编号，cvRound函数将浮点数四舍五入到最近的整数。在这个公式中，log( (double)std::min( base.cols, base.rows ) ) / log(2.) - 2计算的是图像尺寸可以下采样的次数（每次下采样图像尺寸减半），其中 std::min( base.cols, base.rows )取的是图像宽度和高度的最小值，因为只有最小的那个维度减半后还能保持足够的尺寸进行特征点检测。这个计算结果减去2是因为每个octave需要至少2个尺度，所以每个octave至少需要2个图像（即至少需要进行2次下采样）。然后再减去 firstOctave就得到了octaves的数量。 数学公式表示如下： octaves=round(log2(min(M,N))−2)−firstOctaveoctaves = round(log2(min(M,N)) - 2) - firstOctave octaves=round(log2(min(M,N))−2)−firstOctave 其中，M和N分别表示图像的宽度和高度，firstOctave是开始进行尺度空间构建的octave的编号，round函数将浮点数四舍五入到最近的整数。 注意，这只是一个基本的计算方法，实际的计算可能会因为具体算法的实现和参数设置有所不同。 HOG gamma校正 I′(x,y)=I(x,y)gammaI&#x27;(x, y) = I(x, y)^gamma I′(x,y)=I(x,y)gamma Gamma校正可以有效降低图像局部的阴影和光照变化所造成的影响 L2 归一化 : 在计算HOG特征时，我们需要对每个块内的HOG特征向量进行归一化，目的是减小光照变化的影响。一个常见的归一化方法是L2归一化，公式如下： v′=v/(∣∣v∣∣22+eps2)v&#x27; = v /\\sqrt(||v||_2^2 + eps^2) v′=v/(​∣∣v∣∣22​+eps2) 对于一个128∗64128*64128∗64大小的图像，假设cell的大小为8∗88*88∗8，block的大小为16∗1616*1616∗16，梯度被平均分为9个bin，窗口每次滑动的步长为8个像素值。那么横向滑动中，总共有$ 1+(64-16)/8 = 7 个窗口；在纵向滑动中，总共有个窗口；在纵向滑动中，总共有个窗口；在纵向滑动中，总共有 1+(128-16)/8 = 15 $个窗口。对于每个cell，其特征向量对应9维；而对于每个cell，其HOG特征对应9*4=36维；对于要检测的整张图像来说，其HOG特征的维度为36∗7∗15=378036*7*15 = 378036∗7∗15=3780 当你将细胞（cell）大小改为4*4像素时，这将影响图像中的细胞数量、块数量以及最终的HOG特征向量长度。具体如下 细胞单元: 每个细胞单元的大小为4*4像素，这意味着每个细胞单元将有一个9-bin的方向梯度直方图。 块和块的特征向量: 块的大小仍为1616像素，所以一个块现在由44=16个细胞单元组成，因此每个块将有16*9=144个特征。 移动步长和总的块数: 块的移动步长为8像素，所以在64128的图像中，我们可以有 (64/8-1)(128/8-1) = 715 个块（步长8像素对应2个44的细胞，所以我们依然需要减1）。 总的HOG特征向量长度: 因为每个块有144个特征，所以总的HOG特征向量长度为 144 * 7 * 15 = 15120。 因此，将细胞大小从88改为44，将导致HOG特征向量的长度显著增加，从3780增加到15120。这是因为更小的细胞大小能够捕捉到更多的局部信息，但同时也增加了特征的维度和计算复杂性。","categories":[{"name":"机器视觉技术","slug":"机器视觉技术","permalink":"http://jxclbx.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器视觉技术， 期末","slug":"机器视觉技术，-期末","permalink":"http://jxclbx.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%EF%BC%8C-%E6%9C%9F%E6%9C%AB/"}]},{"title":"机器视觉技术期末复习（3）","slug":"机器视觉技术期末复习（3）","date":"2023-06-06T06:17:45.000Z","updated":"2023-06-06T13:48:26.628Z","comments":true,"path":"2023/06/06/机器视觉技术期末复习（3）/","link":"","permalink":"http://jxclbx.github.io/2023/06/06/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%883%EF%BC%89/","excerpt":"","text":"图像处理基础 图像阈值化 各种对阈值进行修正的方法 局部阈值化 带阈值化 多阈值阈值化 半阈值化 阈值检测方法 p率阈值化 物体像素和背景像素各构成一个峰，选取两个峰之间频率最小的灰度值作为阈值 寻找直方图的两个局部极大值，取它们之间的极小值作为阈值 最优阈值化 将图像的直方图用两个或多个正态分布的概率密度函数来近似，获取多个正态分布最大值之间的最小概率处，取距离该处最近的灰度值作为阈值 123456789101112131415161718192021222324function T = iterative_threshold(I) % I: input grayscale image T = mean(I(:)); % Step 1: Initialize the threshold T_new = T; while true % Step 2: Binary segmentation img_binary = I &gt;= T; % Step 3: Compute the means of the two regions m1 = mean(I(img_binary)); m2 = mean(I(~img_binary)); % Step 4: Compute the new threshold T_new = (m1 + m2) / 2.0; % This is the new threshold % Step 5: Repeat until the threshold T does not change anymore if T == T_new break; else T = T_new; end endend OTSU阈值检测 123456789101112131415161718192021222324252627282930313233[count,x]=imhist(G);counts=count/(h*w);% stem(x,counts)%选定一个阈值K,利用K来划分optk=1;optsigma=0;for k=1:256 %像素被分给类1,2的概率 p1=0; for i=1:k p1=p1+counts(i); end %计算k级的累计平均值 mk=0; mk=1*counts(1); for i=2:k mk=mk+i*counts(i); end %全局平均灰度 mg=0; mg=1*counts(1); for i=2:256 mg=mg+i*counts(i); end %计算类间方差 sigma=(mg*p1-mk)^2/(p1*(1-p1)); if sigma&gt;optsigma optsigma=sigma; optk=k; endend 开运算：先腐蚀再膨胀；闭运算：先膨胀再腐蚀 ➢开运算是结构元素B在集合A内完全匹配的并集，完全删除了不能包含结构元素的对象区域 ➢开运算使图像的轮廓变得光滑，断开狭窄的间断，去掉细小的突出物 ➢闭运算：结构元素B在集合A外侧平移，这些平移的并集就是闭运算的结果 ➢闭运算使图像的轮廓变得光滑，但与开运算不同的是，它会将狭窄的缺口连接形成细长的弯口，并填充比结构元素小的洞","categories":[{"name":"机器视觉技术","slug":"机器视觉技术","permalink":"http://jxclbx.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器视觉技术， 期末","slug":"机器视觉技术，-期末","permalink":"http://jxclbx.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%EF%BC%8C-%E6%9C%9F%E6%9C%AB/"}]},{"title":"机器视觉技术期末复习（2）","slug":"机器视觉技术期末复习（2）","date":"2023-06-05T05:45:45.000Z","updated":"2023-06-06T11:14:47.292Z","comments":true,"path":"2023/06/05/机器视觉技术期末复习（2）/","link":"","permalink":"http://jxclbx.github.io/2023/06/05/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%882%EF%BC%89/","excerpt":"","text":"图像预处理 直方图均衡化 算法：直方图均衡化 对于 k(256) 个亮度级、大小为 M × N 的图像，创建长度为 k 的数组 H，初始化为 0 形成图像直方图 H 形成累计直方图 H_c，其中 H_c[0] = H[0]，H_c[p] = H_c[p-1] + H[p]，p = 1, 2, …, k-1 设置 T[p] = round((k-1) / (M * N) * H_c[p])，p = 0, 1, …, k-1 重新扫描图像，根据查找表获得变换结果。 我们通过使用公式 T[p] = round((k-1) / (M * N) * H_c[p]) 来创建一个查找表。这个查找表将帮助我们将原始图像的亮度级映射到新的亮度级。公式中的 k-1 是最大亮度级的值，M × N 是图像的总像素数，而 H_c[p] 表示累计直方图中亮度级小于等于 p 的像素总数。通过这个公式，我们可以计算出每个亮度级在均衡化后的图像中所应该具有的新的亮度级。 用对偶方法完成亮度插值 确定对应于输出图像离散栅格点 在输入图像中原来点的亮度 假设计算在输出图像离散栅格 上像素 (x0′,y0′)\\left(x_0^{\\prime}, y_0^{\\prime}\\right)(x0′​,y0′​) 的亮度值 输入图像中对应点 (x0,y0)\\left(x_0, y_0\\right)(x0​,y0​) 的坐 标可通过像素坐标逆变换得到 (x0,y0)=T−1(x0′,y0′)\\left(x_0, y_0\\right)=\\mathbf{T}^{-1}\\left(x_0^{\\prime}, y_0^{\\prime}\\right) (x0​,y0​)=T−1(x0′​,y0′​) 对输入图像进行亮度插值, 获 得 (x0,y0)/(x0′,y0′)\\left(x_0, y_0\\right) /\\left(x_0^{\\prime}, y_0^{\\prime}\\right)(x0​,y0​)/(x0′​,y0′​) 的亮度值 插值核 对于原图像 f(l,k)f(l, k)f(l,k), 其在 (x,y)(x, y)(x,y) 点的亮度插值结果 可以用卷积公式来表示: fn(x,y)=∑l=−∞∞∑k=−∞∞f(l,k)hn(x−l,y−k)f_n(x, y)=\\sum_{l=-\\infty}^{\\infty} \\sum_{k=-\\infty}^{\\infty} f(l, k) h_n(x-l, y-k) fn​(x,y)=l=−∞∑∞​k=−∞∑∞​f(l,k)hn​(x−l,y−k) hnh_nhn​ 是插值核 (interpolation kernel), 一般只在 很小的区域使用 最近邻插值 赋值像素 (x,y)(\\mathrm{x}, \\mathrm{y})(x,y) 的亮度值为栅格上最近像素点的亮 度值 f1(x,y)=f(round⁡(x),round⁡(y))f_1(x, y)=f(\\operatorname{round}(x), \\operatorname{round}(y)) f1​(x,y)=f(round(x),round(y)) 最近邻插值的位置误差最大是像素大小的一半, 变换后可能呈现阶梯状直边界 线性插值 考虑像素 (x,y)(\\mathrm{x}, \\mathrm{y})(x,y) 的四个相邻点, 假设 亮度在这个邻域内是线性的 f2(x,y)=(1−a)(1−b)f(l,k)l=floor⁡(x)+a(1−b)f(l+1,k)k=floor⁡(y)+b(1−a)f(l,k+1)a=x−l,b=y−l+abf(l+1,k+1)\\begin{aligned} f_2(x, y) &amp; =(1-a)(1-b) f(l, k) &amp; &amp; l=\\operatorname{floor}(x) \\\\ &amp; +a(1-b) f(l+1, k) &amp; &amp; k=\\operatorname{floor}(y) \\\\ &amp; +b(1-a) f(l, k+1) &amp; &amp; a=x-l, b=y-l \\\\ &amp; +a b f(l+1, k+1) &amp; &amp; \\end{aligned} f2​(x,y)​=(1−a)(1−b)f(l,k)+a(1−b)f(l+1,k)+b(1−a)f(l,k+1)+abf(l+1,k+1)​​l=floor(x)k=floor(y)a=x−l,b=y−l​ 线性插值可能引起较小的分辨率 降低和模糊, 但减轻了最近邻插 值中出现的阶梯状直边界 双三次插值 用双三次多项式局部近似亮度函数, 考虑像素 (x,y)(\\mathrm{x}, \\mathrm{y})(x,y) 的16个相邻点, 一维插值核为: h3=(1−2∣x∣2+∣x∣30≤∣x∣&lt;14−8∣x∣+5∣x∣2−∣x∣31≤∣x∣&lt;20 other h_3=\\left(\\begin{array}{cc} 1-2|x|^2+|x|^3 &amp; 0 \\leq|x|&lt;1 \\\\ 4-8|x|+5|x|^2-|x|^3 &amp; 1 \\leq|x|&lt;2 \\\\ 0 &amp; \\text { other } \\end{array}\\right. h3​=⎝⎛​1−2∣x∣2+∣x∣34−8∣x∣+5∣x∣2−∣x∣30​0≤∣x∣&lt;11≤∣x∣&lt;2 other ​ 双三次插值解决了阶梯状边 缘问题和模糊问题, 较好地保持了图像细节, 常用于光栅显示 线性滤波与非线性滤波 线性滤波 假设每个像素上的噪声是一个均值为 0，标准差为 σ\\sigmaσ 的独立随机变量，则可通过多次采集相同的静态景物来获得一幅平均图像: f(m,n)=1k∑i=1kfi(m,n)f(m, n)=\\frac{1}{k} \\sum_{i=1}^k f_i(m, n) f(m,n)=k1​i=1∑k​fi​(m,n) 平均图像中的噪声仍是随机变量，均值为 0，标准差为 σk\\frac{\\sigma}{\\sqrt{k}}k​σ​。若只能获得一幅带有噪声的图像，则通过图像的局部邻域平均完成滤波，这就是均值滤波。 均值滤波属于线性滤波。 如果噪声大小小于图像中感兴趣的最小尺寸，处理结果是可以接受的，但仍存在边缘模糊的问题。 均值滤波是离散卷积的一个特例，对于 3×33 \\times 33×3 的邻域，卷积掩模 hhh 为: h=19[111111111]h=\\frac{1}{9}\\left[\\begin{array}{lll} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] h=91​⎣⎢⎡​111​111​111​⎦⎥⎤​ 其他形式的卷积掩模: h=110[111121111]h=116[121242121]h=\\frac{1}{10}\\left[\\begin{array}{lll} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\quad h=\\frac{1}{16}\\left[\\begin{array}{lll} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 1 \\end{array}\\right] h=101​⎣⎢⎡​111​121​111​⎦⎥⎤​h=161​⎣⎢⎡​121​242​121​⎦⎥⎤​ 高斯滤波：基于高斯函数的形状形成卷积掩模，对于去除服从正态分布的噪声很有效。 一维零均值高斯函数 &gt; 二维零均值高斯函数 h(x,σ)=12πσe−x22σ2h(x,y,σ)=12πσ2e−x2+y22σ2h(x, \\sigma)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{x^2}{2 \\sigma^2}} \\quad h(x, y, \\sigma)=\\frac{1}{2 \\pi \\sigma^2} e^{-\\frac{x^2+y^2}{2 \\sigma^2}} h(x,σ)=2π​σ1​e−2σ2x2​h(x,y,σ)=2πσ21​e−2σ2x2+y2​ 参数 σ\\sigmaσ 决定高斯函数的宽度，进而决定平滑程度。仅使用满足某种标准的像素做平均，从而避免模糊。 高斯函数具有旋转对称性/各向同性，高斯滤波在各个方向上的平滑程度是相同的，从而使后续的边缘检测不会偏向某一方向 高斯函数的傅立叶变换结果仍是高斯函数 高斯函数的可分离性 二维高斯滤波的计算量随模板宽度成线性增长 非线性滤波 线性平滑的问题：图像的边缘被模糊了，非线性平滑方法：根据指定标准，将当前像素邻域分成两个子区域，仅使用邻域中与被处理像素有类似性质的点完成平滑 非线性滤波 限制数据有效性下的平均 中值滤波 限制数据有效性下的平均 设定非法数据范围 [min⁡,max⁡][\\min, \\max][min,max] 只有在 [min⁡,max⁡][\\min, \\max][min,max] 内的像素值才被其邻域的平均所取代 只有有效的数据才对邻域的平均有贡献 卷积掩模: h(i,j)={1f(m+i,n+j)∉[min⁡,max⁡]0其他h(i, j)=\\left\\{\\begin{array}{lc} 1 &amp; f(m+i, n+j) \\notin [\\min, \\max] \\\\ 0 &amp; \\text{其他} \\end{array}\\right. h(i,j)={10​f(m+i,n+j)∈/[min,max]其他​ 中值滤波 基本思想：用像素点邻域灰度值的中值来代替该像素点的灰度值 中值滤波的步骤： 确定掩模大小和掩模中心。 在掩模内将像素点按亮度值大小排序。 选择序列的中间值作为掩模中心的新像素值。 如果像素点数为偶数，中值取排序像素中间两点的平均值。 中值滤波不依赖于邻域内与典型值差别很大的灰度值，可以减少边缘的模糊，对去除脉冲噪声、椒盐噪声非常有效。 边缘检测 拉具有而向同性： 对于图像中的每个像素点，拉普拉斯算子计算其在水平和垂直方向上的二阶导数，并将这两个导数的和作为该像素点的响应值。由于拉普拉斯算子的计算方式只涉及二阶导数，而不考虑梯度的方向，因此它在不同方向上的边缘或纹理特征都会得到相同的响应。这就使得拉普拉斯算子具有各向同性的特性。 连续情况：微分滤波器 离散情况：差分滤波器 公式表示： Robert算子： [−1001][0−110]\\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1\\\\ \\end{bmatrix} \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} [−10​01​][01​−10​] Prewitt算子： [−101−101−101][−1−1−1000111]\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} ⎣⎢⎡​−1−1−1​000​111​⎦⎥⎤​⎣⎢⎡​−101​−101​−101​⎦⎥⎤​ Sobel算子： [−101−202−101][−1−2−1000121]\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{bmatrix} ⎣⎢⎡​−1−2−1​000​121​⎦⎥⎤​⎣⎢⎡​−101​−202​−101​⎦⎥⎤​ 拉普拉斯算子 [010141010]\\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 4 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} ⎣⎢⎡​010​141​010​⎦⎥⎤​ 二阶差分滤波器 滤波器的尺度 用不同参数的高斯滤波器检测边缘，σ越大，图像分辨率越低，检测到的边缘点越少 Canny边缘检测 ➢闭运算：结构元素B在集合A外侧平移，这些平移的并集就是闭运算的结果➢闭运算使图像的轮廓变得光滑，但与开运算不同的是，它会将狭窄的缺口连接形成细长的弯口，并填充比结构元素小的洞 非极大值抑制过程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647function idxLocalMax = cannyFindLocalMaxima(direction,ix,iy,mag)[m,n] = size(mag);switch direction case 1 idx = find((iy&lt;=0 &amp; ix&gt;-iy) | (iy&gt;=0 &amp; ix&lt;-iy)); case 2 idx = find((ix&gt;0 &amp; -iy&gt;=ix) | (ix&lt;0 &amp; -iy&lt;=ix)); case 3 idx = find((ix&lt;=0 &amp; ix&gt;iy) | (ix&gt;=0 &amp; ix&lt;iy)); case 4 idx = find((iy&lt;0 &amp; ix&lt;=iy) | (iy&gt;0 &amp; ix&gt;=iy));end% Exclude the exterior pixelsif ~isempty(idx) v = mod(idx,m); extIdx = (v==1 | v==0 | idx&lt;=m | (idx&gt;(n-1)*m)); idx(extIdx) = [];end%idx 是一维向量，在矩阵转换为向量时，按照列优先进行存储ixv = ix(idx);iyv = iy(idx);gradmag = mag(idx);% Do the linear interpolations for the interior pixelsswitch direction case 1 d = abs(iyv./ixv); gradmag1 = mag(idx+m).*(1-d) + mag(idx+m-1).*d; gradmag2 = mag(idx-m).*(1-d) + mag(idx-m+1).*d; case 2 d = abs(ixv./iyv); gradmag1 = mag(idx-1).*(1-d) + mag(idx+m-1).*d; gradmag2 = mag(idx+1).*(1-d) + mag(idx-m+1).*d; case 3 d = abs(ixv./iyv); gradmag1 = mag(idx-1).*(1-d) + mag(idx-m-1).*d; gradmag2 = mag(idx+1).*(1-d) + mag(idx+m+1).*d; case 4 d = abs(iyv./ixv); gradmag1 = mag(idx-m).*(1-d) + mag(idx-m-1).*d; gradmag2 = mag(idx+m).*(1-d) + mag(idx+m+1).*d;endidxLocalMax = idx(gradmag&gt;=gradmag1 &amp; gradmag&gt;=gradmag2); 阈值化以及获取最终边缘过程 123456789101112131415161718192021222324252627% 根据图像梯度，使用p率阈值化获取高阈值,并计算低阈值% 与阈值相关的参数，设定梯度图像中非边缘比例为0.7，低阈值是高阈值的0.4（super number）PercentOfPixelsNotEdges = .7; % Used for selecting thresholdsThresholdRatio = .4; % Low thresh is this fraction of the high.counts=imhist(magGrad, 64); % 计算直方图highThresh = find(cumsum(counts) &gt; PercentOfPixelsNotEdges*m*n,1,&#x27;first&#x27;) / 64;lowThresh = ThresholdRatio*highThresh;% 通过非极大值抑制和滞后阈值化处理，获得边缘位置idxStrong = [];for dir = 1:4 % 根据像素梯度方向，通过非极大值抑制获取边缘位置，对应算法步骤3 idxLocalMax = cannyFindLocalMaxima(dir,dx,dy,magGrad); % 滞后阈值化处理，对应算法步骤5 idxWeak = idxLocalMax(magGrad(idxLocalMax) &gt; lowThresh); % 经过非极大值抑制保留的边缘位置，图像梯度和低阈值比较，获得弱边缘 e(idxWeak)=1; % e中值为1表示弱边缘 idxStrong = [idxStrong; idxWeak(magGrad(idxWeak) &gt; highThresh)]; % 弱边缘中，图像梯度和高阈值比较，获得强边缘endrstrong = rem(idxStrong-1, m)+1;cstrong = floor((idxStrong-1)/m)+1; % rstrong和cstrong保留着属于强边缘的行和列e = bwselect(e, cstrong, rstrong, 8); % 使用bwselect函数，去除与强边缘不连通的弱边缘thresh = [lowThresh highThresh];eout = e;","categories":[{"name":"机器视觉技术","slug":"机器视觉技术","permalink":"http://jxclbx.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器视觉技术， 期末","slug":"机器视觉技术，-期末","permalink":"http://jxclbx.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%EF%BC%8C-%E6%9C%9F%E6%9C%AB/"}]},{"title":"使用Katex公式","slug":"使用Katex公式","date":"2023-06-05T03:27:41.000Z","updated":"2023-06-05T03:29:52.977Z","comments":true,"path":"2023/06/05/使用Katex公式/","link":"","permalink":"http://jxclbx.github.io/2023/06/05/%E4%BD%BF%E7%94%A8Katex%E5%85%AC%E5%BC%8F/","excerpt":"","text":"首先切换渲染器,在hexo目录下控制台执行以下代码 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-markdown-it-plus --save front-matter中写入 1katex volantis自己的_config中 1234katex: js: https://unpkg.com/katex@0.16.4/dist/katex.min.js # https://unpkg.com/katex@0.15.2/dist/katex.min.js css: https://unpkg.com/katex@0.16.4/dist/katex.min.css # https://unpkg.com/katex@0.15.2/dist/katex.min.css render: https://unpkg.com/katex@0.16.4/dist/contrib/auto-render.min.js # https://unpkg.com/katex@0.15.2/dist/contrib/auto-render.min.js","categories":[],"tags":[]},{"title":"机器视觉技术期末复习（1）","slug":"机器视觉技术期末复习（1）","date":"2023-06-04T16:45:45.000Z","updated":"2023-06-05T05:46:55.513Z","comments":true,"path":"2023/06/05/机器视觉技术期末复习（1）/","link":"","permalink":"http://jxclbx.github.io/2023/06/05/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%881%EF%BC%89/","excerpt":"","text":"常用操作 更改像素值 123456cv::Vec3b pixel = image.at&lt;cv::Vec3b&gt;(y, x);uchar blue = pixel[0];uchar green = pixel[1];uchar red = pixel[2];image.at&lt;cv::Vec3b&gt;(y, x) = cv::Vec3b(255, 0, 0); // 修改像素为蓝色 转换图像颜色空间 12cv::Mat grayImage;cv::cvtColor(image, grayImage, cv::COLOR_BGR2GRAY); 初始化 123456double a[] = &#123;1,2,3,4,5,6,7,8,9&#125;;Mat Ma = (3, 3, CV_64FC1, a);Mat Mb = Ma;Mat Mc;Mc.create(3, 3, CV_64FC1);Mat Md = (Mat_&lt;double&gt;(3,3)&lt;&lt;1,2,3,4,5,6,7,8,9); 在这个例子中，Mat Ma 是一个 3x3 的矩阵，数据类型为 CV_64FC1，并且使用数组 a 来初始化矩阵。让我们逐个解释参数的含义： 3x3：这指定了矩阵的大小，即有3行和3列。 CV_64FC1：这是OpenCV中的数据类型之一，表示矩阵元素为双精度浮点型（64位浮点数），C1 表示通道数为1，即灰度图像。 在OpenCV中，还有其他数据类型可以使用，如下所示： CV_8UC1：8位无符号整数（0-255）的单通道图像。 CV_8UC3：8位无符号整数的三通道图像（彩色图像），通道顺序为BGR。 CV_32FC1：32位浮点型单通道图像。 CV_16SC1：16位有符号整数的单通道图像等等。 这只是一些常见的数据类型，OpenCV还支持其他数据类型和通道数的组合。 a：这是一个数组，用于初始化矩阵。数组 a 应该具有足够的元素，以填充矩阵的所有元素（在这种情况下，共9个元素） Mat_&lt;double&gt;：这是一个模板类，用于指定矩阵的数据类型为双精度浮点型。 (3,3)：这指定了矩阵的大小，即有3行和3列。 &lt;&lt;：这是C++中的位移操作符，用于将给定的元素填充到矩阵中。 1, 2, 3, 4, 5, 6, 7, 8, 9：这是初始化列表，用于提供矩阵的元素值。元素值按行主序排列，即第一行的元素先填充，然后是第二行的元素，以此类推。 这段代码的效果是创建一个 3x3 的双精度浮点型矩阵 Md，并将元素按照行主序填充，即： [123456789]\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{bmatrix} ⎣⎢⎡​147​258​369​⎦⎥⎤​ 访问矩阵元素 1234567891011121314151617181920int main() &#123; cv::Mat Ma = (cv::Mat_&lt;double&gt;(3, 3) &lt;&lt; 1, 2, 3, 4, 5, 6, 7, 8, 9); // 方法1: 使用at函数 std::cout &lt;&lt; Ma.at&lt;double&gt;(2, 2) &lt;&lt; std::endl; Ma.at&lt;double&gt;(1, 1) = 20; // 方法2: 使用Range类 cv::Mat Me = Ma(cv::Range(0, 2), cv::Range(1, 3)); // 方法3: 使用行指针 for (int i = 0; i &lt; Me.rows; i++) &#123; double* pM = Me.ptr&lt;double&gt;(i); for (int j = 0; j &lt; Me.cols; j++) &#123; std::cout &lt;&lt; pM[j] &lt;&lt; std::endl; &#125; &#125; return 0;&#125; 转换颜色 1cvtColor(RGBImg, grayImg, CV_BGR2GRAY) 矩阵操作 12345678Mat Mre;Mre = Ma + 2;Mre = Ma / 4;Mre = Ma.mul(Ma);Mre = Ma * Ma;Mre = Ma + Mb;Mre = Ma.mul(Mb);Mre = Ma * Mb; 数字图像的概念与性质 直方图 直方图给出了图像中各个亮度值出现的概率，一幅k阶图像的直方图由具有k个元素的一维数组表示 直方图给出了图像的全局信息，可用于像素亮度变换、图像分割 算法：计算亮度直方图 创建大小为k的一维数组H 数组H的所有元素初始化为0 对于图像的每一个元素，做如下处理： H[f(m, n)] = H[f(m, n)] + 1 彩色图像 RGB红(Red)、绿(Green)、蓝(Blue)，CMYK青(Cyan)、品红(Magenta)、黄(yellow)、黑(Black) HSB/HSV模型 ➢ 色调(Hue)：表示感知到的色彩，主要指光的波长 ➢ 色饱和度(Saturation)：表示色彩被白光冲淡的程度 ➢ 亮度 (Brightness) ：表示色彩的明暗程度 YIQ模型与YUV模型 ➢ 用于彩色广播电视，Y分量表示亮度；IQ/UV分量携带颜色信息 灰度图像Y = (0.299 * R + 0.587 * G + 0.114 * B) 卷积运算 狄拉克分布(Dirac distribution) /单位冲击函数 𝛿(𝑥, 𝑦)满足 对于所有𝑥, 𝑦 ≠ 0，有𝛿(𝑥, 𝑦) = 0 δ(x,y)={0if x,y≠0∞if x=0 or y=0\\delta(x, y) = \\begin{cases} 0 &amp; \\text{if } x, y \\neq 0 \\\\ \\infty &amp; \\text{if } x = 0 \\text{ or } y = 0 \\end{cases} δ(x,y)={0∞​if x,y=0if x=0 or y=0​ 狄拉克函数的筛选性 ∫−∞∞∫−∞∞f(x,y)δ(x−x0,y−y0) dx dy=f(x0,y0)\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x, y) \\delta(x - x_0, y - y_0) \\, dx \\, dy = f(x_0, y_0) ∫−∞∞​∫−∞∞​f(x,y)δ(x−x0​,y−y0​)dxdy=f(x0​,y0​) 该公式描述了连续图像函数𝑓(𝑥, 𝑦)的采样过程 二维函数𝑓(𝑥, 𝑦)和ℎ(𝑥, 𝑦)的卷积𝑔(𝑥, 𝑦)记为𝑓 ∗ ℎ，由积分定义： g(x,y)=∬−∞+∞f(u,v)h(x−u,y−v) du dvg(x,y) = \\iint_{-\\infty}^{+\\infty} f(u,v)h(x-u, y-v) \\, du \\, dv g(x,y)=∬−∞+∞​f(u,v)h(x−u,y−v)dudv 卷积的性质包括： 交换性： f∗h=h∗ff * h = h * f f∗h=h∗f 结合性： (f∗h1)∗h2=f∗(h1∗h2)(f * h1) * h2 = f * (h1* h2) (f∗h1)∗h2=f∗(h1∗h2) 微分性质： d(f∗h)/dx=f∗(dh/dx)d(f * h) / dx = f * (dh / dx) d(f∗h)/dx=f∗(dh/dx) 狄拉克函数常常被用作一个系统的冲激响应函数，因为与狄拉克函数的卷积不会改变输入信号。 点扩散函数 例如，如果你有一个很尖锐的图像，但你的成像系统有一个较大的PSF，那么你的图像就会看起来模糊，因为每个点都被扩散成了一个斑点。 在另一方面，如果你已知一个图像已经被一个已知的PSF模糊过，那么你可以使用一种称为去卷积的过程来试图恢复原始图像。这是一个更复杂的过程，通常需要更多的计算和一些数学技巧，但它可以在许多情况下帮助提高图像的清晰度。 [0000000001210001343100248420013431000121000000000]\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 4 &amp; 3 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 4 &amp; 8 &amp; 4 &amp; 2 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 4 &amp; 3 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} ⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎡​0000000​0012100​0134310​0248420​0134310​0012100​0000000​⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎤​ 二维离散卷积 g(m,n)=∑k=0M2−1∑l=0N2−1f(m−k,n−l)⋅h(k,l)=∑k=0M1−1∑l=0N1−1f(k,l)⋅h(m−k,n−l)g(m,n) = \\sum_{k=0}^{M2-1} \\sum_{l=0}^{N2-1} f(m-k, n-l) \\cdot h(k,l) \\\\ = \\sum_{k=0}^{M1-1} \\sum_{l=0}^{N1-1} f(k,l) \\cdot h(m-k, n-l) g(m,n)=k=0∑M2−1​l=0∑N2−1​f(m−k,n−l)⋅h(k,l)=k=0∑M1−1​l=0∑N1−1​f(k,l)⋅h(m−k,n−l) g(x,y)=∫−∞+∞∫−∞+∞f(x−a,y−b)⋅h(a,b) da dbg(x,y) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} f(x-a, y-b) \\cdot h(a,b) \\, da \\, db g(x,y)=∫−∞+∞​∫−∞+∞​f(x−a,y−b)⋅h(a,b)dadb f(x,y)尺寸:M1∗N1h(x,y)尺寸:M2∗N2M=0,1,…,M1+M2−2;N=0,1,…,N1+N2−2;f(x,y)尺寸: M1 * N1\\\\ h(x,y)尺寸: M2 * N2\\\\ M = 0,1,…,M1+M2-2;\\\\ N = 0,1,…,N1+N2-2; f(x,y)尺寸:M1∗N1h(x,y)尺寸:M2∗N2M=0,1,…,M1+M2−2;N=0,1,…,N1+N2−2; 降低卷积的时间复杂度 img = imread(‘miss.bmp’);: 从名为 “miss.bmp” 的图像文件中读取图像数据，并将其存储在变量 img 中。这个图像可能是灰度图像或彩色图像。 img = double(imresize(img(:,:,1),[2048,2048]));: 将图像的第一个通道（对于彩色图像）或整个图像（对于灰度图像）转换为 double 类型，并使用 imresize 函数将图像调整为大小为 2048x2048。这样做是为了进行后续的滤波操作。 h = fspecial(‘gaussian’,[128 128],20);: 使用 fspecial 函数创建一个大小为 128x128 的高斯滤波器。这个滤波器具有高斯分布，其中的数值表示滤波器中每个像素的权重。 g1 = uint8(conv2(img,h,‘same’));: 使用 conv2 函数对输入图像 img 应用二维卷积操作，滤波器为 h，并将结果存储在 g1 变量中。‘same’ 参数表示输出图像的大小与输入图像相同。最后，使用 uint8 函数将结果转换为 8 位整数格式。 f_img = fft2(img);: 对输入图像 img 进行二维快速傅里叶变换（FFT），得到频域表示。将结果存储在 f_img 中。 f_h = fft2(h,2048, 2048);: 对滤波器 h 进行二维快速傅里叶变换（FFT），将其调整为与输入图像相同的大小（2048x2048）。将结果存储在 f_h 中。 f_g = f_img .* f_h;: 将输入图像的频域表示 f_img 与滤波器的频域表示 f_h 相乘，得到频域滤波结果 f_g。 g2 = uint8(ifft2(f_g));: 对频域滤波结果 f_g 进行二维逆快速傅里叶变换（IFFT），将其转换回空域表示。最后，使用 uint8 函数将结果转换为 8 位整数格式，并将结果存储在 g2 变量中。 综合起来，这段代码实现了通过直接二维卷积和频域滤波（使用傅里叶变换和卷积定理）两种方法对图像进行高斯滤波，并将结果存储在 g1 和 g2 变量中。 卷积O(n^4) FFTO(n2log2n) 积分图像 算法：积分图像构建 用si,js_{i,j}si,j​表示行方向的累加和，初始化si,−1=0s_{i,-1} = 0si,−1​=0 用iii,jii_{i,j}iii,j​表示积分图像，初始化ii−1,j=0ii_{-1,j} = 0ii−1,j​=0 逐行扫描图像，递归使用下述公式计算每个像素(i,j)(i,j)(i,j)行方向的累加和si,js_{i,j}si,j​，积分图像iii,jii_{i,j}iii,j​的值 si,j=si,j−1+fi,js_{i,j} = s_{i,j - 1} + f_{i,j}si,j​=si,j−1​+fi,j​ iii,j=iii−1,j+s(i,j)ii_{i,j} = ii_{i - 1,j} + s(i,j)iii,j​=iii−1,j​+s(i,j) 扫描一遍图像，完成积分图像构建。 积分图像（Integral Image） 一种描述图像全局信息的矩阵表示方法。 对于原图像fff，积分图像iiiiii中的元素定义为：ii(i,j)=∑k≤i,l≤jf(k,l)ii(i, j) = \\sum_{k \\leq i, l \\leq j} f(k, l) ii(i,j)=k≤i,l≤j∑​f(k,l) 点1的积分 SAT1=Sum(Ra) 点2的积分 SAT2=Sum(Ra)+Sum(Rb) 点3的积分 SAT3=Sum(Ra)+Sum(Rc) 点4的积分 SAT4=Sum(Ra)+Sum(Rb)+Sum(Rc)+Sum(Rd) Sum(Rd)=SAT1+SAT4−SAT2−SAT3","categories":[{"name":"机器视觉技术","slug":"机器视觉技术","permalink":"http://jxclbx.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器视觉技术， 期末","slug":"机器视觉技术，-期末","permalink":"http://jxclbx.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%EF%BC%8C-%E6%9C%9F%E6%9C%AB/"}]},{"title":"Pytorch的奇妙体验","slug":"pytorch的奇妙体验","date":"2023-05-09T13:11:25.000Z","updated":"2023-05-14T11:02:22.160Z","comments":true,"path":"2023/05/09/pytorch的奇妙体验/","link":"","permalink":"http://jxclbx.github.io/2023/05/09/pytorch%E7%9A%84%E5%A5%87%E5%A6%99%E4%BD%93%E9%AA%8C/","excerpt":"","text":"DatasetPyTorch中的Dataset类是一个抽象基类，用于表示数据集。它定义了两个必须实现的方法：__len__() 和 __getitem__()。这个基类是通用的，但它本身无法处理特定类型的数据。因此，当您需要处理特定类型的数据（例如图像、文本等）时，您需要创建一个继承自Dataset类的自定义类，并实现这两个方法，以便根据您的数据加载和处理需求来处理数据。 在您提供的代码示例中，您创建了一个名为ImageDataset的自定义数据集类，它继承自PyTorch的Dataset类。这个类实现了 __len__() 和 __getitem__() 方法，用于处理存储为NumPy格式的图像数据。通过这种方式，您可以使用自定义的数据集类来适应您的特定数据类型和数据处理需求。 总结一下，原因在于PyTorch的Dataset类是一个通用的抽象基类，无法直接处理特n定类型的数据。因此，需要创建自定义数据集类来实现针对特定数据类型的加载和处理。 网络112345678910111213141516class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 32, 5, stride=1) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(32, 64, 5, stride=1) self.fc1 = nn.Linear(64 * 29 * 29, 128) self.fc2 = nn.Linear(128, 7) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 64 * 29 * 29) x = F.relu(self.fc1(x)) x = self.fc2(x) return x 效果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Epoch 1 loss: 1.622, train acc: 0.522, test acc: 0.522Epoch 2 loss: 1.012, train acc: 0.740, test acc: 0.736Epoch 3 loss: 0.707, train acc: 0.811, test acc: 0.814Epoch 4 loss: 0.581, train acc: 0.852, test acc: 0.854Epoch 5 loss: 0.501, train acc: 0.840, test acc: 0.845Epoch 6 loss: 0.416, train acc: 0.886, test acc: 0.884Epoch 7 loss: 0.385, train acc: 0.863, test acc: 0.852Epoch 8 loss: 0.334, train acc: 0.902, test acc: 0.884Epoch 9 loss: 0.315, train acc: 0.922, test acc: 0.902Epoch 10 loss: 0.255, train acc: 0.907, test acc: 0.868Epoch 11 loss: 0.247, train acc: 0.937, test acc: 0.916Epoch 12 loss: 0.193, train acc: 0.956, test acc: 0.924Epoch 13 loss: 0.158, train acc: 0.959, test acc: 0.921Epoch 14 loss: 0.149, train acc: 0.967, test acc: 0.931Epoch 15 loss: 0.132, train acc: 0.961, test acc: 0.914Epoch 16 loss: 0.117, train acc: 0.976, test acc: 0.935Epoch 17 loss: 0.091, train acc: 0.968, test acc: 0.927Epoch 18 loss: 0.084, train acc: 0.980, test acc: 0.933Epoch 19 loss: 0.073, train acc: 0.975, test acc: 0.925Epoch 20 loss: 0.060, train acc: 0.990, test acc: 0.938Epoch 21 loss: 0.060, train acc: 0.978, test acc: 0.922Epoch 22 loss: 0.063, train acc: 0.989, test acc: 0.938Epoch 23 loss: 0.048, train acc: 0.991, test acc: 0.938Epoch 24 loss: 0.042, train acc: 0.990, test acc: 0.935Epoch 25 loss: 0.035, train acc: 0.994, test acc: 0.941Epoch 26 loss: 0.036, train acc: 0.991, test acc: 0.941Epoch 27 loss: 0.029, train acc: 0.993, test acc: 0.938Epoch 28 loss: 0.033, train acc: 0.998, test acc: 0.943Epoch 29 loss: 0.038, train acc: 0.993, test acc: 0.942Epoch 30 loss: 0.026, train acc: 0.998, test acc: 0.945Epoch 31 loss: 0.017, train acc: 0.997, test acc: 0.944Epoch 34 loss: 0.021, train acc: 0.996, test acc: 0.939Epoch 35 loss: 0.017, train acc: 0.990, test acc: 0.936Epoch 36 loss: 0.020, train acc: 0.997, test acc: 0.950Epoch 37 loss: 0.014, train acc: 0.997, test acc: 0.951Epoch 38 loss: 0.011, train acc: 0.998, test acc: 0.945Epoch 39 loss: 0.007, train acc: 1.000, test acc: 0.938Epoch 40 loss: 0.011, train acc: 1.000, test acc: 0.944Epoch 41 loss: 0.007, train acc: 0.995, test acc: 0.940Epoch 42 loss: 0.012, train acc: 1.000, test acc: 0.940Epoch 43 loss: 0.008, train acc: 0.999, test acc: 0.945Epoch 44 loss: 0.009, train acc: 1.000, test acc: 0.946Epoch 45 loss: 0.007, train acc: 0.997, test acc: 0.943Epoch 46 loss: 0.007, train acc: 0.999, test acc: 0.948Epoch 49 loss: 0.009, train acc: 1.000, test acc: 0.948Epoch 50 loss: 0.004, train acc: 1.000, test acc: 0.948Finished Training 这种比较简单的网络结构可能存在一些缺陷： 模型表达能力有限：该网络的深度相对较浅，层数较少，可能无法充分提取输入数据的特征，从而导致模型表达能力不足。 容易出现过拟合：该网络没有使用正则化技术，如dropout等，容易在训练过程中出现过拟合问题，导致模型在测试集上表现不佳。 卷积核尺寸较大：该网络使用的卷积核尺寸为5，可能会导致卷积后的特征图失去一些细节信息，从而降低模型性能。 没有使用预训练模型：该网络是从头开始训练的，没有使用任何预训练模型，可能会导致训练时间较长，模型性能不佳。 以上是该网络可能存在的一些缺陷，可以通过调整网络结构、添加正则化技术、使用更小的卷积核等方式来提高模型性能。 简单提升CNN网络性能增加了更多卷积层，批量标准化层和 Dropout 层来提高性能： 123456789101112131415161718192021222324252627282930313233343536373839class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 32, 3, padding=1) self.bn1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, 3, padding=1) self.bn2 = nn.BatchNorm2d(64) self.pool = nn.MaxPool2d(2, 2) self.conv3 = nn.Conv2d(64, 128, 3, padding=1) self.bn3 = nn.BatchNorm2d(128) self.conv4 = nn.Conv2d(128, 128, 3, padding=1) self.bn4 = nn.BatchNorm2d(128) self.conv5 = nn.Conv2d(128, 256, 3, padding=1) self.bn5 = nn.BatchNorm2d(256) self.conv6 = nn.Conv2d(256, 256, 3, padding=1) self.bn6 = nn.BatchNorm2d(256) self.fc1 = nn.Linear(256 * 8 * 8, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 7) self.dropout = nn.Dropout(p=0.5) def forward(self, x): x = self.pool(F.relu(self.bn1(self.conv1(x)))) x = self.pool(F.relu(self.bn2(self.conv2(x)))) x = F.relu(self.bn3(self.conv3(x))) x = self.pool(F.relu(self.bn4(self.conv4(x)))) x = F.relu(self.bn5(self.conv5(x))) x = self.pool(F.relu(self.bn6(self.conv6(x)))) x = x.view(-1, 256 * 8 * 8) x = self.dropout(F.relu(self.fc1(x))) x = self.dropout(F.relu(self.fc2(x))) x = self.fc3(x) return x 效果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Epoch 1 loss: 1.625, train acc: 0.466, test acc: 0.465Epoch 2 loss: 1.146, train acc: 0.707, test acc: 0.712Epoch 3 loss: 0.603, train acc: 0.895, test acc: 0.883Epoch 4 loss: 0.313, train acc: 0.938, test acc: 0.937Epoch 5 loss: 0.175, train acc: 0.963, test acc: 0.951Epoch 6 loss: 0.135, train acc: 0.970, test acc: 0.956Epoch 7 loss: 0.092, train acc: 0.982, test acc: 0.967Epoch 8 loss: 0.071, train acc: 0.986, test acc: 0.968Epoch 9 loss: 0.055, train acc: 0.988, test acc: 0.971Epoch 10 loss: 0.043, train acc: 0.990, test acc: 0.971Epoch 11 loss: 0.036, train acc: 0.995, test acc: 0.968Epoch 12 loss: 0.028, train acc: 0.994, test acc: 0.979Epoch 13 loss: 0.024, train acc: 0.996, test acc: 0.977Epoch 14 loss: 0.016, train acc: 0.998, test acc: 0.977Epoch 15 loss: 0.017, train acc: 0.997, test acc: 0.978Epoch 16 loss: 0.017, train acc: 0.997, test acc: 0.978Epoch 17 loss: 0.015, train acc: 0.998, test acc: 0.981Epoch 18 loss: 0.013, train acc: 0.998, test acc: 0.979Epoch 19 loss: 0.010, train acc: 0.998, test acc: 0.976Epoch 20 loss: 0.008, train acc: 0.999, test acc: 0.978Epoch 21 loss: 0.008, train acc: 0.999, test acc: 0.980Epoch 22 loss: 0.007, train acc: 0.998, test acc: 0.980Epoch 23 loss: 0.006, train acc: 1.000, test acc: 0.979Epoch 24 loss: 0.005, train acc: 0.999, test acc: 0.978Epoch 25 loss: 0.006, train acc: 0.999, test acc: 0.977Epoch 26 loss: 0.005, train acc: 1.000, test acc: 0.979Epoch 27 loss: 0.005, train acc: 1.000, test acc: 0.977Epoch 28 loss: 0.004, train acc: 0.999, test acc: 0.978Epoch 29 loss: 0.005, train acc: 0.999, test acc: 0.976Epoch 30 loss: 0.004, train acc: 0.999, test acc: 0.982Epoch 31 loss: 0.004, train acc: 1.000, test acc: 0.977Epoch 32 loss: 0.006, train acc: 0.999, test acc: 0.979Epoch 33 loss: 0.005, train acc: 1.000, test acc: 0.978Epoch 34 loss: 0.004, train acc: 0.999, test acc: 0.976Epoch 35 loss: 0.003, train acc: 1.000, test acc: 0.982Epoch 36 loss: 0.003, train acc: 1.000, test acc: 0.977Epoch 37 loss: 0.003, train acc: 1.000, test acc: 0.979Epoch 38 loss: 0.003, train acc: 1.000, test acc: 0.976Epoch 39 loss: 0.003, train acc: 1.000, test acc: 0.979Epoch 40 loss: 0.002, train acc: 1.000, test acc: 0.982Epoch 41 loss: 0.002, train acc: 1.000, test acc: 0.980 网络2（Resnet）拟合速度更快，准确率更高的网络是残差网络（ResNet）。 ResNet是由微软提出的深度残差网络，其主要思想是通过引入残差连接来解决网络退化问题，从而允许网络更深更广，并提高了模型准确率和泛化能力。ResNet常用的版本包括ResNet-18、ResNet-34、ResNet-50、ResNet-101和ResNet-152等。 相比于其他深度神经网络，ResNet的优点有： 更快的训练速度：ResNet通过残差连接解决了梯度消失问题，使得网络可以更深更宽，从而能够更好地拟合数据，提高了训练速度。 更好的泛化能力：残差连接允许网络跨层直接传递信息，避免了信息的损失，使得网络可以更好地学习到数据的特征，提高了模型的泛化能力。 更高的准确率：ResNet通过引入残差连接，使得网络可以更深更宽，提高了模型的表达能力，从而能够更好地拟合数据，提高了模型的准确率。 但是，相比于其他深度神经网络，ResNet占用更多的显存，需要更多的计算资源来训练。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class BasicBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample: residual = self.downsample(x) out += residual out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=7): super(ResNet, self).__init__() self.in_channels = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self.make_layer(block, 64, layers[0]) self.layer2 = self.make_layer(block, 128, layers[1], 2) self.layer3 = self.make_layer(block, 256, layers[2], 2) self.layer4 = self.make_layer(block, 512, layers[3], 2) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512, num_classes) def make_layer(self, block, out_channels, blocks, stride=1): downsample = None if stride != 1 or self.in_channels != out_channels: downsample = nn.Sequential( nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels) ) layers = [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels = out_channels for _ in range(1, blocks): layers.append(block(out_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return x Resnet存在着如下几个问题 网络的深度限制：尽管ResNet的提出解决了深度神经网络的梯度消失问题，但是当网络的深度增加时，ResNet仍然会出现梯度消失和梯度爆炸的问题。这限制了ResNet的深度。 特征重复利用不充分：在ResNet中，残差块中的特征并没有充分地被重复利用。相对于DenseNet，ResNet的特征传递方式是逐级传递，即特征只在当前和下一个块之间传递，而不是在所有块之间传递。 训练时间较长：由于ResNet是一个非常深的网络，所以它的训练时间会比较长，特别是当训练数据集很大时。 1234567891011121314151617181920212223242526272829Epoch 1 loss: 1.333, train acc: 0.757, test acc: 0.734Epoch 2 loss: 0.492, train acc: 0.938, test acc: 0.904Epoch 3 loss: 0.171, train acc: 0.982, test acc: 0.951Epoch 4 loss: 0.061, train acc: 0.996, test acc: 0.956Epoch 5 loss: 0.026, train acc: 0.999, test acc: 0.960Epoch 6 loss: 0.012, train acc: 1.000, test acc: 0.960Epoch 7 loss: 0.006, train acc: 1.000, test acc: 0.962Epoch 8 loss: 0.005, train acc: 1.000, test acc: 0.964Epoch 9 loss: 0.004, train acc: 1.000, test acc: 0.963Epoch 10 loss: 0.003, train acc: 1.000, test acc: 0.964Epoch 11 loss: 0.003, train acc: 1.000, test acc: 0.964Epoch 12 loss: 0.002, train acc: 1.000, test acc: 0.964Epoch 13 loss: 0.002, train acc: 1.000, test acc: 0.964Epoch 14 loss: 0.002, train acc: 1.000, test acc: 0.963Epoch 15 loss: 0.002, train acc: 1.000, test acc: 0.964Epoch 16 loss: 0.002, train acc: 1.000, test acc: 0.963Epoch 17 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 18 loss: 0.001, train acc: 1.000, test acc: 0.962Epoch 19 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 20 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 21 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 22 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 23 loss: 0.001, train acc: 1.000, test acc: 0.964Epoch 24 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 25 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 26 loss: 0.001, train acc: 1.000, test acc: 0.964Epoch 27 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 28 loss: 0.001, train acc: 1.000, test acc: 0.963Epoch 29 loss: 0.001, train acc: 1.000, test acc: 0.963 网络3（DenseNet）在DenseNet中，每个层的输出都会被连接到后续所有层的输入中，这使得每个层都可以直接获取到之前所有层的特征图，从而增加了特征重用的程度，避免了特征的浪费。在DenseNet中，特征图之间的连接可以使用张量拼接（concatenate）来实现。 具体地，DenseNet可以由多个密集块（Dense Block）和一个全局池化层（Global Pooling Layer）组成。每个密集块由多个卷积层和一个批量归一化层（Batch Normalization Layer）组成，卷积层的输出将被拼接到后续所有卷积层的输入中。全局池化层的输出将被送入一个全连接层和一个Softmax层中进行分类。 DenseNet的优点包括： 特征重用程度高：在DenseNet中，每个层都可以直接获取到之前所有层的特征图，从而增加了特征重用的程度，避免了特征的浪费。 模型参数较少：在DenseNet中，由于特征图之间的连接可以使用张量拼接来实现，所以模型参数较少。 准确率高：DenseNet在图像分类等任务上表现出色，达到了当时最好的性能。 然而，DenseNet也有一些缺点，如模型计算量较大、模型结构复杂等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182class DenseLayer(nn.Module): def __init__(self, in_channels, growth_rate): super(DenseLayer, self).__init__() self.bn1 = nn.BatchNorm2d(in_channels) self.conv1 = nn.Conv2d(in_channels, growth_rate * 4, kernel_size=1, stride=1, bias=False) self.bn2 = nn.BatchNorm2d(growth_rate * 4) self.conv2 = nn.Conv2d(growth_rate * 4, growth_rate, kernel_size=3, stride=1, padding=1, bias=False) def forward(self, x): out = self.bn1(x) out = F.relu(out) out = self.conv1(out) out = self.bn2(out) out = F.relu(out) out = self.conv2(out) out = torch.cat((x, out), 1) return outclass DenseBlock(nn.Module): def __init__(self, in_channels, growth_rate, num_layers): super(DenseBlock, self).__init__() self.layers = nn.ModuleList([DenseLayer(in_channels + i * growth_rate, growth_rate) for i in range(num_layers)]) def forward(self, x): for layer in self.layers: x = layer(x) return xclass TransitionLayer(nn.Module): def __init__(self, in_channels, out_channels): super(TransitionLayer, self).__init__() self.bn = nn.BatchNorm2d(in_channels) self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) def forward(self, x): out = self.bn(x) out = F.relu(out) out = self.conv(out) out = F.avg_pool2d(out, 2) return outclass DenseNet(nn.Module): def __init__(self, growth_rate, block_config, num_classes=7): super(DenseNet, self).__init__() self.conv1 = nn.Conv2d(3, growth_rate * 2, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(growth_rate * 2) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) in_channels = growth_rate * 2 self.dense_blocks = nn.ModuleList() self.transition_layers = nn.ModuleList() for i, num_layers in enumerate(block_config): dense_block = DenseBlock(in_channels, growth_rate, num_layers) self.dense_blocks.append(dense_block) in_channels += num_layers * growth_rate if i != len(block_config) - 1: transition_layer = TransitionLayer(in_channels, in_channels // 2) self.transition_layers.append(transition_layer) in_channels = in_channels // 2 self.bn2 = nn.BatchNorm2d(in_channels) self.fc = nn.Linear(in_channels, num_classes) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) for i, dense_block in enumerate(self.dense_blocks): x = dense_block(x) if i != len(self.dense_blocks) - 1: x = self.transition_layers[i](x) x = self.bn2(x) x = F.relu(x) x = F.adaptive_avg_pool2 效果如下； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Epoch 1 loss: 1.401, train acc: 0.684, test acc: 0.669Epoch 2 loss: 0.556, train acc: 0.945, test acc: 0.921Epoch 3 loss: 0.188, train acc: 0.984, test acc: 0.951Epoch 4 loss: 0.074, train acc: 0.997, test acc: 0.956Epoch 5 loss: 0.024, train acc: 1.000, test acc: 0.965Epoch 6 loss: 0.012, train acc: 1.000, test acc: 0.966Epoch 7 loss: 0.007, train acc: 1.000, test acc: 0.964Epoch 8 loss: 0.005, train acc: 1.000, test acc: 0.965Epoch 9 loss: 0.004, train acc: 1.000, test acc: 0.967Epoch 10 loss: 0.003, train acc: 1.000, test acc: 0.966Epoch 11 loss: 0.003, train acc: 1.000, test acc: 0.965Epoch 12 loss: 0.003, train acc: 1.000, test acc: 0.966Epoch 13 loss: 0.002, train acc: 1.000, test acc: 0.967Epoch 14 loss: 0.002, train acc: 1.000, test acc: 0.967Epoch 15 loss: 0.002, train acc: 1.000, test acc: 0.966Epoch 16 loss: 0.002, train acc: 1.000, test acc: 0.967Epoch 17 loss: 0.002, train acc: 1.000, test acc: 0.966Epoch 18 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 19 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 20 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 21 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 22 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 23 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 24 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 25 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 26 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 27 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 28 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 29 loss: 0.001, train acc: 1.000, test acc: 0.966Epoch 30 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 31 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 34 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 36 loss: 0.001, train acc: 1.000, test acc: 0.968Epoch 37 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 38 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 39 loss: 0.001, train acc: 1.000, test acc: 0.967Epoch 40 loss: 0.001, train acc: 1.000, test acc: 0.968Epoch 41 loss: 0.001, train acc: 1.000, test acc: 0.968Epoch 42 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 43 loss: 0.000, train acc: 1.000, test acc: 0.967Epoch 44 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 45 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 46 loss: 0.000, train acc: 1.000, test acc: 0.967Epoch 47 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 48 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 49 loss: 0.000, train acc: 1.000, test acc: 0.968Epoch 50 loss: 0.000, train acc: 1.000, test acc: 0.968Finished Training 提高准确度方法： 调整超参数：尝试不同的学习率、批量大小、优化器和权重衰减。可以使用网格搜索或随机搜索找到最佳超参数组合。同时，可以考虑使用学习率调度器逐渐降低学习率。 更深或更宽的模型：尝试使用更复杂的模型，如更深或更宽的 ResNet、DenseNet 或其他现代架构。通常，更复杂的模型具有更大的表示能力，可以提高性能。 数据增强：使用数据增强技术，如随机旋转、翻转、缩放、剪裁和亮度调整等，可以扩展训练数据集并提高模型泛化能力。 正则化：使用正则化技术，如 L1 或 L2 正则化、Dropout 或 Batch Normalization，可以减轻过拟合并提高模型泛化能力。 更多数据：如果可能，尝试收集更多的训练数据。更多的数据有助于模型学习更多的特征，从而提高准确性。 早停法：在验证集上监控模型性能，当性能不再提高时，提前停止训练。这有助于防止过拟合。 预训练模型：使用预训练的模型作为初始模型，然后在您的数据集上进行微调。这样可以利用在大型数据集上学到的特征，加速收敛并提高性能。 集成方法：训练多个模型并将它们的输出结合起来。这可以是简单的平均，或者可以使用更复杂的技术，如投票或模型堆叠。这有助于提高模型的稳定性和准确性。 数据增强与上文不同，我在加入高斯噪声的基础上加入了图像旋转变换来提高模型的泛化能力 损失函数要改善模型的训练效果，您可以尝试使用其他损失函数。这里我使用Label Smoothing Cross Entropy损失。可以提高模型的泛化能力，因为它在训练过程中为模型提供了额外的正则化。 12345678910111213141516class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, eps=0.1, reduction=&#x27;mean&#x27;): super(LabelSmoothingCrossEntropy, self).__init__() self.eps = eps self.reduction = reduction def forward(self, output, target): c = output.size(1) log_preds = F.log_softmax(output, dim=1) loss = F.nll_loss(log_preds, target, reduction=self.reduction) smooth_loss = -log_preds.mean(dim=1) if self.reduction == &#x27;mean&#x27;: smooth_loss = smooth_loss.mean() elif self.reduction == &#x27;sum&#x27;: smooth_loss = smooth_loss.sum() return loss * (1 - self.eps) + smooth_loss * self.eps BATCH_SIZEBatch size的选择对模型的训练效果和收敛速度有很大影响。然而，并没有一个固定的答案来确定最佳的batch size。 训练稳定性和收敛速度 ：较大的batch size可以让梯度下降过程更稳定，因为每个batch的平均梯度对噪声更不敏感。然而，大的batch size可能会导致训练过程收敛速度变慢，因为每次迭代更新权重的次数减少了。相反，较小的batch size可以提高训练速度，因为每个epoch内权重更新的次数增加，但可能会导致训练过程不稳定，这是由于小batch size中噪声较多。 泛化能力 ：有研究表明，较小的batch size可能有助于提高模型的泛化能力。这可能是因为较小的batch size在训练过程中引入了随机性和正则化，从而防止模型过拟合。 训练时间 ：较大的batch size可以减少每个epoch所需的迭代次数，从而减少同步和数据传输的开销，提高计算资源的利用率。但是，如果batch size过大，可能会导致GPU内存不足，进而影响训练速度。","categories":[{"name":"CV","slug":"CV","permalink":"http://jxclbx.github.io/categories/CV/"}],"tags":[{"name":"专业课","slug":"专业课","permalink":"http://jxclbx.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"CV","slug":"CV","permalink":"http://jxclbx.github.io/tags/CV/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://jxclbx.github.io/tags/Pytorch/"}]},{"title":"网页设计与制作作业2","slug":"网页设计与制作作业2","date":"2023-05-09T05:55:09.000Z","updated":"2023-05-09T16:37:30.258Z","comments":true,"path":"2023/05/09/网页设计与制作作业2/","link":"","permalink":"http://jxclbx.github.io/2023/05/09/%E7%BD%91%E9%A1%B5%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%B6%E4%BD%9C%E4%BD%9C%E4%B8%9A2/","excerpt":"","text":"阐述 CSS 盒模型原理CSS盒模型是CSS中最基础的概念之一，它描述了一个元素在网页中的呈现方式。盒模型将一个元素看做是一个矩形盒子，由四个部分组成：内容区域（content）、内边距区域（padding）、边框区域（border）和外边距区域（margin）。 内容区域，也就是元素内部实际显示内容的区域。比如div元素，它的内容区域就是它所包含的文字、图片、列表等内容。 内边距区域，它是在内容区域和边框之间的空白区域，它为元素的内容提供空间。可以设置背景颜色、背景图片等样式属性。可以通过设置padding属性来调整内边距的大小。 边框区域，它包围着内边距和内容区域，并为元素提供了可见的边界。可以通过设置border属性来调整边框的大小、样式和颜色。 外边距区域，它是元素边框与相邻元素边框之间的空白区域。可以通过设置margin属性来调整外边距的大小。 举例阐述 CSS 中几种常用的选择器 通配符选择器（Wildcard Selector）：使用 * 选择器可以匹配页面上的所有元素。 标签选择器（Tag Selector）：使用标签名称作为选择器，例如 div、p、h1，可以选择指定类型的元素。 ID 选择器（ID Selector）：使用 # 加上元素的唯一标识符，例如 #myId，可以选择具有指定 ID 的元素。ID 应在页面中是唯一的。 类选择器（Class Selector）：使用 . 加上类名，例如 .myClass，可以选择具有指定类名的元素。多个元素可以共享相同的类。 伪类选择器（Pseudo-Class Selector）：使用 : 加上伪类名，例如 :hover、:first-child，可以选择特定状态或位置的元素。伪类可以根据用户交互或元素的位置进行选择。 具体效果剪文件夹内代码 举例阐述 CSS 几种定位方式绝对定位（Absolute Positioning）：使用 position: absolute; 将元素的位置相对于其最近的已定位祖先元素进行定位，如果没有已定位的祖先元素，则相对于文档的窗口进行定位。通过使用 top、bottom、left 和 right 属性，可以精确控制元素在页面上的位置。 相对定位（Relative Positioning）：使用 position: relative; 将元素相对于其正常位置进行定位。相对定位不会改变其他元素的布局，仍会占据原来的空间。通过使用 top、bottom、left 和 right 属性，可以相对于元素的正常位置调整其位置。 固定定位（Fixed Positioning）：使用 position: fixed; 将元素的位置相对于视口（浏览器窗口）进行定位，无论页面滚动与否，元素都会保持在固定的位置。通过使用 top、bottom、left 和 right 属性，可以确定元素在视口中的具体位置。 浮动定位（Float Positioning）：使用 float 属性将元素从正常的文档流中浮动到指定的位置。浮动元素可以向左或向右浮动，并使其周围的元素环绕在其周围。浮动通常用于创建多列布局。 做一个 CSS 3动画使用了 CSS 的样式和关键帧动画来实现了一个动画效果，通过定义样式和应用动画属性创建了一个渐变放大和旋转的圆形元素，并在动画结束后以渐变放大的方式显示了一段文字。 linear-gradient()：渐变函数用于创建线性渐变背景。在 .circle 类的样式中，使用了 linear-gradient(45deg, #ff8a00, #e52e71, #4c4c4c) 来创建一个从斜角 45 度开始的线性渐变背景。 border-radius：用于设置元素的边框圆角。在 .circle 类的样式中，使用了 border-radius: 50% 来将元素设置为一个圆形。 transform：用于对元素进行变换，例如旋转、缩放、平移等。在 .circle 和 .text 类的样式中，使用了 transform: translate(-50%, -50%) scale(0) 来设置元素的初始位置和大小。 animation 和 @keyframes：这是 CSS3 中用于创建动画的特性。在 .circle 和 .text 类的样式中，使用了 animation 属性来指定动画名称、持续时间、缓动函数和循环次数等。而 @keyframes 则定义了关键帧动画的不同阶段和样式","categories":[{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/categories/HTML/"}],"tags":[{"name":"专业课","slug":"专业课","permalink":"http://jxclbx.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/tags/HTML/"},{"name":"网页","slug":"网页","permalink":"http://jxclbx.github.io/tags/%E7%BD%91%E9%A1%B5/"}]},{"title":"快速使用VSCode编写HTML文件","slug":"快速使用VSCode编写HTML文件","date":"2023-05-09T05:33:11.000Z","updated":"2023-05-09T16:36:32.559Z","comments":true,"path":"2023/05/09/快速使用VSCode编写HTML文件/","link":"","permalink":"http://jxclbx.github.io/2023/05/09/%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8VSCode%E7%BC%96%E5%86%99HTML%E6%96%87%E4%BB%B6/","excerpt":"","text":"安装安装相关插件——搜索html,安装如下插件 再次打开插件商店，搜索open in browser 回到你的html文件，ctrl+s保存文件，然后shift+alt+b，在弹出的窗口中输入open in ,选择open in Other Browsers,如图(或者右键文件空白处，如图二红箭头所指向的两个，一个是用默认浏览器，一个是用其他浏览器。","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://jxclbx.github.io/tags/vscode/"},{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/tags/HTML/"}]},{"title":"Hexo博客自定义页面跳过渲染","slug":"Hexo博客自定义页面跳过渲染","date":"2023-05-08T07:39:01.000Z","updated":"2023-05-08T07:47:01.039Z","comments":true,"path":"2023/05/08/Hexo博客自定义页面跳过渲染/","link":"","permalink":"http://jxclbx.github.io/2023/05/08/Hexo%E5%8D%9A%E5%AE%A2%E8%87%AA%E5%AE%9A%E4%B9%89%E9%A1%B5%E9%9D%A2%E8%B7%B3%E8%BF%87%E6%B8%B2%E6%9F%93/","excerpt":"","text":"Hexo自定义原理Hexo 系列的博客中的文章都是经Hexo的主题渲染的静态网页。所以Hexo博客大部分都呈现出一种高度的统一化与规范化。不过 Hexo 提供了跳过渲染功能，使得我们可以直接在博客中放入自定义网页。 比如在博客中放入图片、自定义404.html、自定义About页面、简历等 创建自定义网页网页可以是自己编写的，也可以是别人现成的源码（下载喜欢的页面）。 网页编写完成后，在Hexo\\source目录下创建一个文件夹（文件夹名称任意，比如我创建的是about这个文件夹，部署完成后，访问http://mrlsm.github.io/about即可看到效果，依此类推） 将 html 文件放置于此文件夹，并重命名为 index.html 。 跳过渲染跳过渲染有下述两种方法： 在自定义页面的开头添加如下： 123---layout: false--- 添加该指令后，执行 hexo g命令时便会跳过该 index.html文件，使得index.html不受当前 hexo 主题影响，完全是一个独立的网页，如果网页引用了 css 或 js，css 和 js 需使用外链或者将css js 文件放入index.html同目录下引用。 引用图片亦是如此 在_config.yml文件中设置skip_render使用编辑器打开 Hexo 目录下的_config.yml文件，找到skip_render skip_render一般有以下四种常用参数： 跳过source目录下的 test.html: skip_render: test.html 跳过source目录下 test 文件夹内所有文件：skip_render: test&#x2F;* 跳过source目录下 test 文件夹内所有文件包括子文件夹以及子文件夹内的文件：skip_render: test&#x2F;**跳过多个路径： 123skip_render:- curriculumVitae/**- DecrypeMusic/** 最后执行 1hexo g -d","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://jxclbx.github.io/tags/hexo/"},{"name":"音乐","slug":"音乐","permalink":"http://jxclbx.github.io/tags/%E9%9F%B3%E4%B9%90/"}]},{"title":"算法设计实验题目","slug":"算法设计实验题目","date":"2023-05-06T08:37:01.000Z","updated":"2023-05-09T16:37:12.169Z","comments":true,"path":"2023/05/06/算法设计实验题目/","link":"","permalink":"http://jxclbx.github.io/2023/05/06/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%AE%9E%E9%AA%8C%E9%A2%98%E7%9B%AE/","excerpt":"","text":"GS算法匹配小狗狗 忘了 BFS DFS树的层序遍历（3月29日） 4.5没上课","categories":[{"name":"算法设计课程","slug":"算法设计课程","permalink":"http://jxclbx.github.io/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E8%AF%BE%E7%A8%8B/"}],"tags":[{"name":"专业课","slug":"专业课","permalink":"http://jxclbx.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"算法","slug":"算法","permalink":"http://jxclbx.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Anaconda配置PyTorch环境","slug":"Anaconda配置PyTorch环境","date":"2023-05-05T14:36:39.000Z","updated":"2023-05-09T16:37:57.813Z","comments":true,"path":"2023/05/05/Anaconda配置PyTorch环境/","link":"","permalink":"http://jxclbx.github.io/2023/05/05/Anaconda%E9%85%8D%E7%BD%AEPyTorch%E7%8E%AF%E5%A2%83/","excerpt":"","text":"在Anaconda下安装Pytorch安装pytorch，有两种办法，一是pip，二是conda。不管什么样的方法，首先，都要安装最新的anaconda。 安装AnacondaAnaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。里面所包含的Jupyter Notebook是数据挖掘领域中最热门的工具。(例如Kaggle网站) 没安装Anaconda的小伙伴可以参考以下安装链接： https://blog.csdn.net/qq_4521807/article/details/112442577 安装Pytorch打开Anaconda Prompt在命令行格式下，输入代码，完成调用清华镜像、建立pytorch环境、安装pytorch、测试pytorch过程 调用清华镜像1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ 1conda config --set show_channel_urls yes 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 这个配置好以后，以后再安装其他的软件如果要用到清华镜像源网站就不用了重新配置了。 注意！如果切换镜像后当出现下载不了的情况，就先切换默认源，然后再修改另一个可以使用的conda源（一定要先恢复默认，再换另一个！！！） 切回默认源：1conda config --remove-key channels 创建Pytorch环境说真的，别在命令行里费那劲了，给你个GUI为嘛不用呢 1conda create -n pytorch python=3.7 查看环境是否安装成功 1conda info --envs 下载Pytorch根据自己的安装版本，在Pytorch官网寻找安装命令代码：Pytorch官网：https://pytorch.org/ 查看CUDA版本nvidia-smi solve environment需要比较长的时间，换了清华源之后就基本不需要挂梯子了 等待安装完成出现done 此时在vscode中可以在python解释器选择里看到pytorch环境的解释器： 按下F1键选择解释器，结束。","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"anaconda","slug":"anaconda","permalink":"http://jxclbx.github.io/tags/anaconda/"},{"name":"python","slug":"python","permalink":"http://jxclbx.github.io/tags/python/"}]},{"title":"耳机评测系列（一）--并不是东洋650，而是东洋1840","slug":"并不是东洋650，而是东洋1840——","date":"2023-05-04T14:59:49.000Z","updated":"2023-05-04T15:22:56.000Z","comments":true,"path":"2023/05/04/并不是东洋650，而是东洋1840——/","link":"","permalink":"http://jxclbx.github.io/2023/05/04/%E5%B9%B6%E4%B8%8D%E6%98%AF%E4%B8%9C%E6%B4%8B650%EF%BC%8C%E8%80%8C%E6%98%AF%E4%B8%9C%E6%B4%8B1840%E2%80%94%E2%80%94/","excerpt":"","text":"敬请期待","categories":[{"name":"评测","slug":"评测","permalink":"http://jxclbx.github.io/categories/%E8%AF%84%E6%B5%8B/"}],"tags":[{"name":"铁三角， Audio Technica","slug":"铁三角，-Audio-Technica","permalink":"http://jxclbx.github.io/tags/%E9%93%81%E4%B8%89%E8%A7%92%EF%BC%8C-Audio-Technica/"},{"name":"耳机","slug":"耳机","permalink":"http://jxclbx.github.io/tags/%E8%80%B3%E6%9C%BA/"},{"name":"评测","slug":"评测","permalink":"http://jxclbx.github.io/tags/%E8%AF%84%E6%B5%8B/"}]},{"title":"Anaconda常用操作","slug":"Anaconda常用配置","date":"2023-05-04T09:30:21.000Z","updated":"2023-05-05T15:47:22.000Z","comments":true,"path":"2023/05/04/Anaconda常用配置/","link":"","permalink":"http://jxclbx.github.io/2023/05/04/Anaconda%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"在使用 python anaconda时，经常会用到很多常用操作，记录下来，方便以后更好地使用： CondaConda既是一个包管理器又是一个环境管理器。你肯定知道包管理器，它可以帮你发现和查看包。但是如果当我们想要安装一个包，但是这个包只支持跟我们目前使用的python不同的版本时。你只需要几行命令，就可以搭建起一个可以运行另外python版本的环境。这就是conda环境管理器的强大功能。 Conda常用命令1conda update conda # 升级conda 12conda create -n pytorch1 python=3 Astroid Babel#创建基于python3 ，包含Astroid 和 Babel 包，称为pytorch1的新环境，在/envs/bunnies文件夹里 123# 查看当前可用环境conda env list conda info --envs 123# 切换工作环境conda activate baseconda deactivate 123456# 复制一个环境conda create -n flowers --clone snowflakes # 重新命名：先 clone 一份 new name 的环境；删除 old name 的环境；conda create -n tf --clone rcnn # 克隆conda remove -n rcnn --all # 删除conda info -e # 重新查看环境 123# 删除一个环境conda remove -n flowers --allconda info -e # 查看是否环境已经成功被移除 12345678# 管理Python环境# 检查python版本conda search --full --name python conda search python # 使用模糊匹配 # 安装一个新的版本 conda create -n snakes python=3# 查看已经安装的环境 conda info -e 123456789101112# 管理包# 查看当前环境中包含的包和其版本列表 conda list # 查找一个包conda search beautifulsoup4 # 安装一个包conda install --name bunnies beautifulsoup4 # 你必须告诉conda你要安装环境的名字（-n bunies）否则它将会被安装到当前环境中 # 使用 pip 安装一个包，并可使用 conda list 进行查看；pip install see conda list 123# 删除整个anaconda rm -rf ~/miniconda OR rm -rf ~/anaconda # 直接删除整个文件夹，并去除.bashrc 中的配置文件即可，对环境影响较少；","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"anaconda","slug":"anaconda","permalink":"http://jxclbx.github.io/tags/anaconda/"},{"name":"python","slug":"python","permalink":"http://jxclbx.github.io/tags/python/"}]},{"title":"站内搜索","slug":"站内搜索","date":"2023-05-03T05:46:39.000Z","updated":"2023-05-03T05:53:12.000Z","comments":true,"path":"2023/05/03/站内搜索/","link":"","permalink":"http://jxclbx.github.io/2023/05/03/%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2/","excerpt":"","text":"站内搜索配置代码 1234567891011121314# To use hexo search, you need to install the following plugins:# npm i hexo-generator-json-contentsearch: enable: true service: hexo # hexo, algolia, meilisearch algolia: searchAsYouType: true # If false, triggers the search only on submit. hitsPerPage: 5 # Set the number of hits per page. placeholder: &#x27;Search...&#x27; # The placeholder text of the input. meilisearch: placeholder: &#x27;Search...&#x27; searchKey: &#x27;&#x27; indexName: &#x27;&#x27; hostUrl: &#x27;&#x27; 显然这段没啥用，因为我们不需要使用algolia搜索等等 你需要安装 hexo-generator-json-content，并根据它的文档去做相应配置。 修改 主题配置文件 。 123search: enable: true service: hexo","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://jxclbx.github.io/tags/hexo/"},{"name":"站内搜索","slug":"站内搜索","permalink":"http://jxclbx.github.io/tags/%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2/"}]},{"title":"Github+jsDelivr搭建个人图床","slug":"Github-jsDelivr搭建个人图床","date":"2023-05-02T16:13:51.000Z","updated":"2023-05-03T04:01:06.000Z","comments":true,"path":"2023/05/03/Github-jsDelivr搭建个人图床/","link":"","permalink":"http://jxclbx.github.io/2023/05/03/Github-jsDelivr%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%9B%BE%E5%BA%8A/","excerpt":"","text":"Github+jsDelivr图床经常写博文的朋友对床图肯定不陌生。使用markdown撰写博客，将图片放在床图网站生成外链统一管理，这样一份博文就可以发布在不同的平台，也避免了不同网页对同一张图片引用的。不过免费的床图网站有时不稳定，付费价格又都不便宜。 最近了解到了Github+jsDelivr的方式搭建个人床图，稳定快速免费。 搭建方法也比较简单，本文默认已经： 有Github账号 通过SSH与本地Git绑定 掌握基本的Git操作 那么，搭建床图仅需三步。 在GIthub建立一个仓库在创建GitHub仓库并与本地Git绑定中已经完成 将本地图片push到仓库 先将建好的仓库clone到本地 将需要上传的图片添加到对应文件夹 git push 图片就是保存在github仓库，每个仓库有1个G的容量限制。1个G？不叫事，那能存很多图片。如果你图片存满，那再建个新仓库就是了。 Github的资源在国内加载速度比较慢，所以需要用到CDN技术来加速。 CDN的全称是Content Delivery Network，即内容分发网络。CDN是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。 jsDelivr(https://cdn.jsdelivr.net)就是一种免费且快速的CDN，通过jsDelivr引用资源GIthub图片资源，即可实现图片加速。所以接下来的第三步，改写一下链接就搞定了。主题内部也是用了这种方法。 通过jsDelivr引用资源使用方法： 1https://cdn.jsdelivr.net/gh/你的用户名/你的仓库名@发布的版本号/文件路径 此处 1https://cdn.jsdelivr.net/gh/jxclbx/blogImages/文件路径 例如访问https://cdn.jsdelivr.net/gh/jxclbx/blogImages/imageSource/bg.jpg 得到如下效果： 图床接入 markdown的图片URL可以填入网络地址，并且paste image插件所输出的格式就是标准的markdown格式，而不是hexo的引用图片格式，我们只需在写完一篇blog后，多加入一步上传图片到github的步骤即可。 在merge后，直接将md文件中的url做替换，加入 1https://cdn.jsdelivr.net/gh/jxclbx/blogImages/imagePost/ 即可完成。","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"图床","slug":"图床","permalink":"http://jxclbx.github.io/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"jsDelivr","slug":"jsDelivr","permalink":"http://jxclbx.github.io/tags/jsDelivr/"},{"name":"git","slug":"git","permalink":"http://jxclbx.github.io/tags/git/"}]},{"title":"创建GitHub仓库并与本地Git绑定","slug":"创建GitHub仓库并与本地Git绑定","date":"2023-05-02T13:40:34.000Z","updated":"2023-05-03T03:54:06.000Z","comments":true,"path":"2023/05/02/创建GitHub仓库并与本地Git绑定/","link":"","permalink":"http://jxclbx.github.io/2023/05/02/%E5%88%9B%E5%BB%BAGitHub%E4%BB%93%E5%BA%93%E5%B9%B6%E4%B8%8E%E6%9C%AC%E5%9C%B0Git%E7%BB%91%E5%AE%9A/","excerpt":"","text":"为了创建一个图床 有Github账号 通过SSH与本地Git绑定 掌握基本的Git操作 这三步是缺一不可的，现在先来将SSH绑定git 创建一个新的仓库我们点击“New repository”创建一个新的仓库： 得到SSH地址 绑定SSH双击git-bash.exe，在本地创建ssh key： 1ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; 然后成功后会在User文件夹对应的用户下创建.ssh文件夹，其中有一个id_rsa.pub文件，我们复制其中的key: 之后返回github，进入 Account Settings（账户配置），左边选择SSH and GPG Keys选项 其中的title随便填，下面的粘贴在你电脑上生成的key。点击添加之后，则添加成功： 验证是否绑定本地成功，在git-bash中验证，输入指令： 1ssh -T git@github.com 如果第一次执行该指令，则会提示是否continue继续，如果我们输入yes就会看到成功信息： 1ssh -T git@github.com github不支持shell这个可以忽略。 1Hi jxclbx! You&#x27;ve successfully authenticated, but GitHub does not provide shell access. Git操作由于GitHub每次执行commit操作时，都会记录username和email，所以要设置它们： 12git config --global user.name &quot;jxclbx&quot;git config --global user.email &quot;13001392777@163.com&quot; Clone到本地1git clone git@github.com:jxclbx/blogImages.git 此时在目录下会到一个隐藏的.git文件夹，该文件夹是Git用来跟踪管理版本库的，然后将所有文件添加到仓库，并提交文件： 1git add . 1git commit -m &quot; &quot; Add &amp; Commitgit commit 是 Git 版本控制系统中用于保存本地仓库更改的命令。当你在本地 Git 仓库中更改文件时，可以使用 git commit 创建一个新的快照并将其添加到 Git 历史记录中。这有助于跟踪你随着时间推移所做的更改并与其他人共同开发同一项目。 要使用 git commit，你首先需要使用 git add 将要提交的更改加入到暂存区中。这告诉 Git 你想要包含在提交中的更改内容。一旦你将更改加入到暂存区中，就可以使用以下命令将其提交： -m 标志用于添加提交信息，描述你所做的更改。编写清晰和描述性的提交信息非常重要，这样其他开发人员可以轻松地理解你所做的更改。 如果你想在提交中包含工作目录中的所有更改，可以使用以下命令： 1git commit -a -m &quot;提交信息&quot; -a 标志告诉 Git 自动将仓库中所有已修改或已删除的更改加入到暂存区中。 提交完成后，可以将其推送到远程仓库以与他人共享更改或保留更改的备份。 暂存区暂存区是 Git 版本控制系统中的一个概念，它是介于工作目录和 Git 仓库之间的一个中间状态，也被称为 Git 的“索引”（index）。它是用于临时存储已修改或已删除文件的地方，以便在下一次提交时包含这些更改。 暂存区在本地 Git 仓库的 .git 目录中的 index 文件中。每次使用 git add 命令将文件添加到暂存区时，Git 会将这些更改写入 index 文件中。在执行 git commit 命令之前，你可以使用 git status 命令来查看哪些文件已经被添加到暂存区，哪些文件还未被添加。 需要注意的是，暂存区只是一个中间状态，只有执行 git commit 命令将暂存区中的更改提交到 Git 仓库后，这些更改才会被永久保存下来。如果你在暂存区中添加了一个文件，但之后又对该文件进行了修改，那么只有重新使用 git add 命令将该文件添加到暂存区，之后再使用 git commit 命令才能将最新的更改提交到 Git 仓库中。 关于远程仓库：remote在 Git 中，remote 表示远程仓库的别名或名称。当你从远程仓库中获取代码或将代码推送到远程仓库时，需要使用远程仓库的名称。为了方便起见，Git 允许为每个远程仓库分配一个别名，这个别名就是 remote。 在 git remote add 命令中，remote 参数指定了新远程仓库的名称或别名，origin 就是一个常用的远程仓库别名。在这个命令中，origin 将被用作指向远程仓库的别名，而 git@github.com:jxclbx/blogImages.git 则是该远程仓库的 URL。这个命令将把远程仓库 git@github.com:jxclbx/blogImages.git 添加到本地 Git 仓库中，并将其命名为 origin。 在添加远程仓库后，你可以使用 git remote -v 命令查看所有已添加的远程仓库，包括它们的别名和 URL。 这个错误意味着在尝试将本地 Git 仓库连接到远程仓库时，Git 发现已经存在一个名为 origin 的远程仓库。这通常发生在你尝试在已经存在 origin 的情况下再次运行 git remote add origin 命令，或者在克隆仓库时指定了一个与 origin 名称相同的远程仓库。 要解决这个错误，可以尝试以下方法： 1git remote rm origin 本地仓库建立完成此时我们的本地仓库就建立好了。 然后我们的本地仓库要关联GitHub的仓库，直接将本地仓库关联远程GitHub仓库地址即可 1git remote add origin git@github.com:jxclbx/blogImages.git 上传本地代码至GitHub下面要上传本地代码至GitHub，但是前提是远程仓库不能使空的，所以我们在远程仓库中创建一个README.md的文件： 本地仓库也创建一个一模一样的README.md文件即可，然后使用git pull origin master远程更新一下。 然后我们在原来的git bash中提交本地仓库中的web工程源代码： 1git push -u origin master error: src refspec master does not match any确认本地 Git 仓库中是否存在名为 master 的分支。使用以下命令查看本地分支： 1git branch 如果 master 分支不存在，则可以使用以下命令创建该分支： 1git checkout -b master Pull request 出现 There isn’t anything to compare.请移步另一篇文章。至此，已经绑定以及创建仓库。","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"git","slug":"git","permalink":"http://jxclbx.github.io/tags/git/"},{"name":"ssh","slug":"ssh","permalink":"http://jxclbx.github.io/tags/ssh/"}]},{"title":"你好，五月！","slug":"你好，五月！","date":"2023-04-28T06:11:48.000Z","updated":"2023-08-27T13:30:46.370Z","comments":true,"path":"2023/04/28/你好，五月！/","link":"","permalink":"http://jxclbx.github.io/2023/04/28/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E4%BA%94%E6%9C%88%EF%BC%81/","excerpt":"","text":"五月 一个寻常的五月，往往以一个寻常的错误开篇。 寻常的不能再寻常 寻常到……front-matter的Headimg千万不要忘了打后缀名，本地路径打上了后缀名也会因为外部链接没法引用这个路径下的image而无法显示。。最后投奔了图床…… 开始 真正的五月在北京开启，爬展花花卡丁车 好在五月是这样的一个轻松充实的基调 （富士真好玩.jpg 让我们和四月说一声再见吧，希望五月的风带来我想见的你，愿你想要的明天，都会如约而至。","categories":[{"name":"浮生","slug":"浮生","permalink":"http://jxclbx.github.io/categories/%E6%B5%AE%E7%94%9F/"}],"tags":[{"name":"日记","slug":"日记","permalink":"http://jxclbx.github.io/tags/%E6%97%A5%E8%AE%B0/"}]},{"title":"VS_Code配置Python解释器","slug":"VS-Code配置Python解释器","date":"2023-04-26T13:21:53.000Z","updated":"2023-05-09T05:36:24.202Z","comments":true,"path":"2023/04/26/VS-Code配置Python解释器/","link":"","permalink":"http://jxclbx.github.io/2023/04/26/VS-Code%E9%85%8D%E7%BD%AEPython%E8%A7%A3%E9%87%8A%E5%99%A8/","excerpt":"","text":"我们熟悉的老朋友VS Code今天cv2莫名其妙报错，经过一番排查，得到是Python自身出了问题，故记录一下VSC与anaconda配置其的过程 VSCode首先，我们需要在环境变量中添加 12C:\\Users\\13001\\AppData\\Local\\Programs\\Python\\Python37C:\\Users\\13001\\AppData\\Local\\Programs\\Python\\Python37\\Scripts 再在VSCode中，Ctrl+Shift+P 或者 View &gt; Command Palette，打开命令面板输入 Python: Select Interpreter 选择Python的安装路径 可以使用上方的刷新符号来更新已经卸载的python版本的状态使其消失 选择好解释器后，就可以愉快的开始使用了","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"python","slug":"python","permalink":"http://jxclbx.github.io/tags/python/"},{"name":"垃圾py","slug":"垃圾py","permalink":"http://jxclbx.github.io/tags/%E5%9E%83%E5%9C%BEpy/"},{"name":"vscode","slug":"vscode","permalink":"http://jxclbx.github.io/tags/vscode/"}]},{"title":"公开前：配置主题","slug":"公开前：配置主题","date":"2023-04-23T15:35:32.000Z","updated":"2023-05-03T03:35:48.000Z","comments":true,"path":"2023/04/23/公开前：配置主题/","link":"","permalink":"http://jxclbx.github.io/2023/04/23/%E5%85%AC%E5%BC%80%E5%89%8D%EF%BC%9A%E9%85%8D%E7%BD%AE%E4%B8%BB%E9%A2%98/","excerpt":"","text":"首先，非常激动，非常开心，我打开了撰写blog的大门。 配置主题首先是更改主题 1npm i hexo-theme-volantis 这步是为了将主题安置到 blog\\node_modules\\hexo-theme-volantis 关于背景图片的替换将图片放置在 node_modules\\hexo-theme-volantis\\source\\images 中，再将 12345678cover: height_scheme: full # full, half layout_scheme: dock # blank (留白), search (搜索), dock (坞), featured (精选), focus (焦点) display: home: true archive: true others: false # can be written in front-matter &#x27;cover: true&#x27; background: /images/bg.jpg # background image 的background字段更改为相对路径即可 关于引用图片hexo-renderer-marked 3.1.0 引入了一个新的选项，其允许你无需使用 asset_img 标签插件就可以在 markdown 中嵌入图片 然后再在 _config.yml中更改代码块 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 后在 _posts 文件夹中新建与post名相同的文件夹即可","categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"自言自语","slug":"自言自语","permalink":"http://jxclbx.github.io/tags/%E8%87%AA%E8%A8%80%E8%87%AA%E8%AF%AD/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-04-22T15:54:53.000Z","updated":"2023-05-02T15:55:08.000Z","comments":true,"path":"2023/04/22/hello-world/","link":"","permalink":"http://jxclbx.github.io/2023/04/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"技术","slug":"技术","permalink":"http://jxclbx.github.io/categories/%E6%8A%80%E6%9C%AF/"},{"name":"浮生","slug":"浮生","permalink":"http://jxclbx.github.io/categories/%E6%B5%AE%E7%94%9F/"},{"name":"机器视觉技术","slug":"机器视觉技术","permalink":"http://jxclbx.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF/"},{"name":"CV","slug":"CV","permalink":"http://jxclbx.github.io/categories/CV/"},{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/categories/HTML/"},{"name":"算法设计课程","slug":"算法设计课程","permalink":"http://jxclbx.github.io/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E8%AF%BE%E7%A8%8B/"},{"name":"评测","slug":"评测","permalink":"http://jxclbx.github.io/categories/%E8%AF%84%E6%B5%8B/"}],"tags":[{"name":"python","slug":"python","permalink":"http://jxclbx.github.io/tags/python/"},{"name":"HTML","slug":"HTML","permalink":"http://jxclbx.github.io/tags/HTML/"},{"name":"日记","slug":"日记","permalink":"http://jxclbx.github.io/tags/%E6%97%A5%E8%AE%B0/"},{"name":"机器视觉技术， 期末","slug":"机器视觉技术，-期末","permalink":"http://jxclbx.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E6%8A%80%E6%9C%AF%EF%BC%8C-%E6%9C%9F%E6%9C%AB/"},{"name":"专业课","slug":"专业课","permalink":"http://jxclbx.github.io/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"CV","slug":"CV","permalink":"http://jxclbx.github.io/tags/CV/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://jxclbx.github.io/tags/Pytorch/"},{"name":"网页","slug":"网页","permalink":"http://jxclbx.github.io/tags/%E7%BD%91%E9%A1%B5/"},{"name":"vscode","slug":"vscode","permalink":"http://jxclbx.github.io/tags/vscode/"},{"name":"hexo","slug":"hexo","permalink":"http://jxclbx.github.io/tags/hexo/"},{"name":"音乐","slug":"音乐","permalink":"http://jxclbx.github.io/tags/%E9%9F%B3%E4%B9%90/"},{"name":"算法","slug":"算法","permalink":"http://jxclbx.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"anaconda","slug":"anaconda","permalink":"http://jxclbx.github.io/tags/anaconda/"},{"name":"铁三角， Audio Technica","slug":"铁三角，-Audio-Technica","permalink":"http://jxclbx.github.io/tags/%E9%93%81%E4%B8%89%E8%A7%92%EF%BC%8C-Audio-Technica/"},{"name":"耳机","slug":"耳机","permalink":"http://jxclbx.github.io/tags/%E8%80%B3%E6%9C%BA/"},{"name":"评测","slug":"评测","permalink":"http://jxclbx.github.io/tags/%E8%AF%84%E6%B5%8B/"},{"name":"站内搜索","slug":"站内搜索","permalink":"http://jxclbx.github.io/tags/%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2/"},{"name":"图床","slug":"图床","permalink":"http://jxclbx.github.io/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"jsDelivr","slug":"jsDelivr","permalink":"http://jxclbx.github.io/tags/jsDelivr/"},{"name":"git","slug":"git","permalink":"http://jxclbx.github.io/tags/git/"},{"name":"ssh","slug":"ssh","permalink":"http://jxclbx.github.io/tags/ssh/"},{"name":"垃圾py","slug":"垃圾py","permalink":"http://jxclbx.github.io/tags/%E5%9E%83%E5%9C%BEpy/"},{"name":"自言自语","slug":"自言自语","permalink":"http://jxclbx.github.io/tags/%E8%87%AA%E8%A8%80%E8%87%AA%E8%AF%AD/"}]}